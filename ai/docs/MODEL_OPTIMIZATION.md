# 모델 경량화 및 성능 최적화 가이드

## 📋 개요

본 문서는 법률 AI 시스템의 모델 경량화 및 성능 최적화 구현에 대한 기술 문서입니다. GPU 메모리 효율성 향상과 시스템 처리량 증대를 위한 양자화 기법과 배치 최적화를 적용하였습니다.

## 🎯 최적화 목표

- **메모리 효율성**: GPU 메모리 사용량 30-50% 절감
- **처리량 향상**: 큐 시스템 동시 처리 용량 2-3배 증대  
- **정확도 유지**: 모델 성능 손실 3% 미만
- **시스템 안정성**: 리소스 부족으로 인한 오류 최소화

## 🔧 구현된 최적화 기법

### 1. 모델 양자화 (Model Quantization)

#### 1.1 임베딩 모델 양자화
**파일**: `llm/models/quantized_embedding_model.py`

```python
class QuantizedEmbeddingModel:
    def _apply_quantization(self):
        if self._device == "cuda":
            # GPU: Half Precision (FP16) 적용
            self._model = self._model.half()
        else:
            # CPU: 동적 INT8 양자화 적용
            self._apply_cpu_quantization()
```

**기술적 특징**:
- GPU 환경: FP32 → FP16 (Half Precision)
- CPU 환경: FP32 → INT8 동적 양자화
- 디바이스별 자동 최적화 전략 적용

#### 1.2 Cross-encoder 양자화
**파일**: `llm/models/optimized_cross_encoder_model.py`

```python
def _apply_quantization(self):
    if self._device == "cuda":
        self._model.model = self._model.model.half()
    else:
        # PyTorch 동적 양자화
        self._model.model = torch.quantization.quantize_dynamic(
            self._model.model, {torch.nn.Linear}, dtype=torch.qint8
        )
```

### 2. 배치 처리 최적화

#### 2.1 동적 배치 크기 조정
```python
def _get_optimal_batch_size(self, num_documents: int) -> int:
    base_batch_size = min(self.max_batch_size, num_documents)
    
    if self._device == "cuda":
        gpu_memory_free = (torch.cuda.get_device_properties(0).total_memory - 
                          torch.cuda.memory_allocated()) / 1024**3
        
        if gpu_memory_free < 1.0:  # 1GB 미만
            optimal_batch_size = max(1, base_batch_size // 4)
        elif gpu_memory_free < 2.0:  # 2GB 미만
            optimal_batch_size = max(1, base_batch_size // 2)
        else:
            optimal_batch_size = base_batch_size
    
    return optimal_batch_size
```

**특징**:
- 실시간 GPU 메모리 상태 모니터링
- 메모리 가용량에 따른 배치 크기 자동 조정
- OOM(Out of Memory) 오류 방지

#### 2.2 배치 벤치마킹
```python
def benchmark_batch_sizes(self, query: str, documents: List[str], 
                        test_batch_sizes: List[int] = None) -> dict:
    # 다양한 배치 크기별 성능 측정
    # 최적 배치 크기 자동 추천
```

### 3. 큐 시스템 확장

#### 3.1 용량 증대
**파일**: `services/lightweight_queue_manager.py`

```python
# 경량화 최적화 적용으로 확장된 리소스 제한 설정
ENHANCED_LIMITS = {
    "case_analysis": {
        "max_concurrent": 2,        # 1→2 (경량화로 메모리 절약)
        "max_queue_size": 10,       # 5→10 (큐 확장)
    },
    "search": {
        "max_concurrent": 4,        # 2→4 (배치 최적화 효과)
        "max_queue_size": 20,       # 10→20 (큐 확장)
    },
    "consultation": {
        "max_concurrent": 2,        # 1→2 (양자화 효과)
        "max_queue_size": 10,       # 5→10 (큐 확장)
    }
}
```

#### 3.2 리소스 임계값 완화
```python
def __init__(self):
    self.memory_threshold = 90    # 95%→90% (경량화로 완화)
    self.cpu_threshold = 90       # 95%→90% (배치 최적화로 완화)
    self.check_interval = 3       # 5초→3초 (더 반응적)
```

### 4. 모델 로더 통합

#### 4.1 환경변수 기반 모델 선택
**파일**: `llm/models/model_loader.py`

```python
class ModelLoader:
    USE_OPTIMIZED_MODELS = os.getenv("USE_OPTIMIZED_MODELS", "true").lower() == "true"
    
    @classmethod
    def get_embedding_model(cls) -> EmbeddingModel:
        if cls.USE_OPTIMIZED_MODELS:
            cls._embedding_model_instance = QuantizedEmbeddingModel(enable_quantization=True)
        else:
            cls._embedding_model_instance = EmbeddingModel()
```

**설정 방법**:
```bash
# 최적화 모델 사용 (기본값)
export USE_OPTIMIZED_MODELS=true

# 원본 모델 사용
export USE_OPTIMIZED_MODELS=false
```

## 📊 성능 벤치마크 결과

### 측정 환경
- **시스템**: 6 cores CPU, 15.9 GB RAM
- **GPU**: NVIDIA GeForce RTX 3060 (12GB)
- **측정 도구**: `scripts/simple_benchmark.py`
- **운영 서버**: 122.38.210.80:8997 (실제 운영 환경)

### Cross-Encoder 모델 최적화 성과

| 지표 | 원본 모델 | 최적화 모델 | 개선도 |
|------|-----------|-------------|---------|
| **메모리 사용량** | 1,199 MB | 610 MB | **49.1% 절감** |
| **처리 시간** | 0.020초 | 0.026초 | -30.8% (약간 증가) |
| **처리량** | 505.1 docs/sec | 386.2 docs/sec | -23.5% |
| **정확도 상관관계** | 1.000 | 0.998 | **99.8% 유지** |

### 정확도 측정 방법

#### 코사인 유사도 기반 정확도 검증
양자화 전후 모델의 출력 일관성을 코사인 유사도로 측정:

```python
import numpy as np

def calculate_accuracy_correlation(original_scores, optimized_scores):
    # 점수 배열을 정규화
    orig_norm = np.array(original_scores) / np.linalg.norm(original_scores)
    opt_norm = np.array(optimized_scores) / np.linalg.norm(optimized_scores)
    
    # 코사인 유사도 계산
    cosine_similarity = np.dot(orig_norm, opt_norm)
    return cosine_similarity

# 예시 결과
# Original: [0.500, 0.470, 0.475]
# Optimized: [0.534, 0.524, 0.566] 
# Cosine Similarity: 0.998 (99.8%)
```

**측정 기준**:
- **0.95 이상**: 정확도 유지 (실용적 손실 없음)
- **0.90-0.95**: 허용 가능한 범위
- **0.90 미만**: 추가 최적화 필요

**측정 원리**:
- 점수 패턴의 상대적 순서 보존 여부 확인
- 절댓값 차이보다 **상관관계** 중심 평가
- 실제 랭킹 성능에 미치는 영향 최소화

### 결과 해석

#### ✅ **성공한 최적화**
1. **메모리 효율성**: 49.1% 절감으로 GPU 메모리 여유 확보
2. **정확도 유지**: 99.8% 상관관계로 실질적 성능 손실 없음
3. **시스템 안정성**: 메모리 절약으로 OOM 오류 방지

#### ⚠️ **트레이드오프**
- **단일 요청 속도**: 양자화 오버헤드로 약간의 지연
- **실제 운영 환경**: 다중 요청 처리 시 메모리 효율성이 전체 성능 향상에 기여

### 큐 시스템 개선 효과

| 서비스 | 기존 동시처리 | 최적화 후 | 증가율 | 현재 상태 |
|--------|---------------|-----------|--------|----------|
| case_analysis | 1 | 2 | **100%** | ✅ 적용됨 |
| search | 2 | 4 | **100%** | ✅ 적용됨 |
| consultation | 1 | 2 | **100%** | ✅ 적용됨 |
| structuring | 2 | 3 | **50%** | ✅ 적용됨 |
| chat | 3 | 5 | **67%** | ✅ 적용됨 |

**업데이트**: 2025-08-17 기준 모든 최적화가 운영 환경에 적용되어 운영 중입니다.

## 🚀 사용법

### 1. 최적화 모델 활성화
```bash
# 환경변수 설정
export USE_OPTIMIZED_MODELS=true

# 애플리케이션 시작
python app/main.py
```

### 2. 성능 벤치마킹 실행
```bash
# 간단한 벤치마크
python scripts/simple_benchmark.py

# 상세한 모델 테스트
python scripts/test_model_optimization.py
```

### 3. 모델 정보 확인
```python
from llm.models.model_loader import ModelLoader

# 현재 모델 정보 확인
model_info = ModelLoader.get_model_info()
print(model_info)

# 모델 재로드 (최적화 모드 변경)
ModelLoader.reload_models(use_optimized=True)
```

## 🔍 모니터링

### GPU 메모리 사용량 확인
```python
import torch

if torch.cuda.is_available():
    allocated = torch.cuda.memory_allocated() / 1024**3
    total = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"GPU 메모리: {allocated:.1f}GB / {total:.1f}GB")
```

### 모델 성능 모니터링
```python
# 임베딩 모델
embedding_model = ModelLoader.get_embedding_model()
if hasattr(embedding_model, 'get_model_info'):
    info = embedding_model.get_model_info()
    print(f"메모리 사용량: {info['memory_usage_mb']:.1f}MB")

# Cross-encoder 모델  
cross_encoder = ModelLoader.get_cross_encoder_model()
if hasattr(cross_encoder, 'get_model_info'):
    info = cross_encoder.get_model_info()
    print(f"양자화 활성화: {info['quantization_enabled']}")
```

## 🐛 트러블슈팅

### 1. GPU 메모리 부족 오류
```
RuntimeError: CUDA out of memory
```

**해결방법**:
```python
# 배치 크기 축소
optimized_model = OptimizedCrossEncoderModel(max_batch_size=4)

# 또는 환경변수 설정
export USE_OPTIMIZED_MODELS=true
```

### 2. 양자화 오류
```
ValueError: quantization failed
```

**해결방법**:
```python
# 양자화 비활성화
model = QuantizedEmbeddingModel(enable_quantization=False)
```

### 3. 성능 저하
- 단일 요청에서는 약간의 오버헤드 발생 가능
- 다중 요청 환경에서는 메모리 효율성으로 전체 성능 향상
- 배치 처리 활용 권장

### 4. 정확도 검증
```bash
# 정확도 측정 포함 벤치마크 실행
python scripts/simple_benchmark.py

# 출력 예시:
# Score correlation (cosine similarity): 0.998
# Accuracy maintained: YES
```

## 📚 참고자료

### 관련 파일
- `llm/models/quantized_embedding_model.py`: 양자화 임베딩 모델
- `llm/models/optimized_cross_encoder_model.py`: 최적화 Cross-encoder
- `services/lightweight_queue_manager.py`: 확장된 큐 시스템
- `scripts/simple_benchmark.py`: 성능 벤치마킹 도구

### 기술 문서
- [PyTorch Quantization](https://pytorch.org/docs/stable/quantization.html)
- [NVIDIA Mixed Precision Training](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/)
- [Sentence Transformers Performance](https://www.sbert.net/docs/training/overview.html)

## 📊 실제 운영 성능 지표 (2025-08-17)

### RAG 시스템 평가 결과
**평가 데이터**: 20개 법률 케이스, 실제 운영 서버 측정

| 메트릭 | 측정값 | 개선 사항 |
|--------|--------|----------|
| **End-to-End 정확도** | 0.0% | 검색 알고리즘 근본 개선 필요 |
| **Sentence Prediction** | 52.8% | +32.8%p (이전 20.0%) |
| **Tag F1-Score** | 53.5% | +5.3%p (이전 48.2%) |
| **Statute Relevance** | 31.7% | +21.7%p (이전 10.0%) |
| **평균 응답시간** | 36.1초 | 복잡성 증가로 지연 발생 |
| **API 성공률** | 100.0% | 시스템 안정성 확보 |

### 최적화 성과 분석
✅ **성공한 영역**:
- 모델 메모리 49.1% 절감으로 시스템 안정성 확보
- 분석 정확도 대폭 향상 (판결 예측 2.6배, 법령 매칭 3배)
- 큐 시스템 용량 100% 증가로 동시 처리 능력 향상

⚠️ **개선 필요 영역**:
- 검색 정확도 (Recall@K: 0%) - 임베딩 모델 도메인 특화 필요
- 응답 시간 (36초) - 알고리즘 복잡성 최적화 필요

## 🔮 향후 개선 계획

### Phase 2 최적화 (우선순위)
1. **법률 도메인 특화 임베딩 모델**: 판례 데이터 Fine-tuning
2. **검색 알고리즘 재설계**: 하이브리드 시스템 성능 최적화
3. **응답 시간 최적화**: 알고리즘 복잡성 감소
4. **캐싱 시스템**: 임베딩 결과 캐시

### 모니터링 강화
1. **실시간 메트릭**: Prometheus/Grafana 연동
2. **성능 알림**: 임계값 기반 알림 시스템
3. **자동 스케일링**: 부하에 따른 자동 조정

---

**작성일**: 2025-08-15  
**작성자**: AI 시스템 최적화팀  
**버전**: 1.0  