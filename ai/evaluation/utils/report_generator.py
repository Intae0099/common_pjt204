"""
RAG í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ëª¨ë“ˆ
"""
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List

logger = logging.getLogger(__name__)

class ReportGenerator:
    """RAG í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±ê¸°"""
    
    def __init__(self, reports_dir: str):
        self.reports_dir = Path(reports_dir)
        self.reports_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_reports(self, 
                        evaluation_results: Dict[str, Any],
                        config: Dict[str, Any]) -> Dict[str, str]:
        """ì „ì²´ ë¦¬í¬íŠ¸ ìƒì„±"""
        timestamp = datetime.now()
        
        # í‰ê°€ ì •ë³´ ì¶”ì¶œ
        total_cases = len(evaluation_results.get('case_results', []))
        k_values = config.get('evaluation', {}).get('k_values', [1, 5, 10])
        k_str = '-'.join(map(str, k_values))
        
        # ì˜ë¯¸ìˆëŠ” íŒŒì¼ëª… ìƒì„±: rag-eval_20cases_k1-5-10_2025-01-13_15-30
        date_str = timestamp.strftime("%Y-%m-%d")
        time_str = timestamp.strftime("%H-%M")
        report_name = f"rag-eval_{total_cases}cases_k{k_str}_{date_str}_{time_str}"
        
        # í•˜ìœ„ í´ë” êµ¬ì¡° ìƒì„±: reports/2025-01-13/
        date_folder = self.reports_dir / date_str
        date_folder.mkdir(parents=True, exist_ok=True)
        
        # JSON ë¦¬í¬íŠ¸ ìƒì„±
        metrics_file = date_folder / f"{report_name}_metrics.json"
        self._generate_json_report(evaluation_results, config, metrics_file, timestamp)
        
        # ë§ˆí¬ë‹¤ìš´ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        summary_file = date_folder / f"{report_name}_summary.md"
        self._generate_markdown_report(evaluation_results, config, summary_file, timestamp)
        
        # ìµœì‹  ë¦¬í¬íŠ¸ ë§í¬ ìƒì„± (ë£¨íŠ¸ reports í´ë”ì—)
        try:
            latest_metrics = self.reports_dir / "latest-evaluation_metrics.json"
            latest_summary = self.reports_dir / "latest-evaluation_summary.md"
            
            # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ì‚­ì œ
            if latest_metrics.exists():
                latest_metrics.unlink()
            if latest_summary.exists():
                latest_summary.unlink()
            
            # ë³µì‚¬
            import shutil
            shutil.copy2(metrics_file, latest_metrics)
            shutil.copy2(summary_file, latest_summary)
            
        except Exception as e:
            logger.warning(f"ìµœì‹  ë¦¬í¬íŠ¸ ë§í¬ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return {
            'metrics_file': str(metrics_file),
            'summary_file': str(summary_file)
        }
    
    def _generate_json_report(self, 
                            results: Dict[str, Any], 
                            config: Dict[str, Any],
                            output_file: Path,
                            timestamp: datetime):
        """JSON í˜•ì‹ ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        report_data = {
            "evaluation_summary": {
                "total_cases": len(results.get('case_results', [])),
                "timestamp": timestamp.isoformat(),
                "config": {
                    "k_values": config.get('evaluation', {}).get('k_values', [1, 3, 5]),
                    "api_endpoint": config.get('api', {}).get('base_url', ''),
                    "timeout_seconds": config.get('api', {}).get('timeout_seconds', 30)
                }
            }
        }
        
        # ë©”íŠ¸ë¦­ ë°ì´í„° ì¶”ê°€
        if 'aggregated_metrics' in results:
            aggregated = results['aggregated_metrics']
            
            if 'search_metrics' in aggregated:
                report_data['search_metrics'] = aggregated['search_metrics']
            
            if 'analysis_metrics' in aggregated:
                report_data['analysis_metrics'] = aggregated['analysis_metrics']
        
        # ì „ì²´ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ê°€
        if 'overall_metrics' in results:
            report_data['overall_metrics'] = results['overall_metrics']
        
        # ì¼€ì´ìŠ¤ë³„ ìƒì„¸ ê²°ê³¼ ì¶”ê°€
        if config.get('output', {}).get('include_case_details', True):
            report_data['case_results'] = results.get('case_results', [])
        
        # ì—ëŸ¬ ì •ë³´ ì¶”ê°€
        if 'errors' in results:
            report_data['errors'] = results['errors']
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"JSON ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {output_file}")
            
        except Exception as e:
            logger.error(f"JSON ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def _generate_markdown_report(self, 
                                results: Dict[str, Any], 
                                config: Dict[str, Any],
                                output_file: Path,
                                timestamp: datetime):
        """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        md_content = []
        
        # í—¤ë”
        md_content.append("# RAG ì„±ëŠ¥ í‰ê°€ ê²°ê³¼\n\n")
        md_content.append(f"**í‰ê°€ ì¼ì‹œ**: {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\n")
        md_content.append(f"**ì´ ì¼€ì´ìŠ¤**: {len(results.get('case_results', []))}ê°œ\n")
        
        # ì „ì²´ ì„±ëŠ¥ ìš”ì•½
        overall = results.get('overall_metrics', {})
        if overall:
            runtime = overall.get('total_runtime_s', 0)
            md_content.append(f"**í‰ê°€ ì†Œìš”ì‹œê°„**: {runtime:.1f}ì´ˆ\n\n")
        
        # í•µì‹¬ ì§€í‘œ ì„¹ì…˜
        md_content.append("## ğŸ“Š í•µì‹¬ ì§€í‘œ\n\n")
        
        # ê²€ìƒ‰ ì„±ëŠ¥
        search_metrics = results.get('aggregated_metrics', {}).get('search_metrics', {})
        if search_metrics:
            md_content.append("### ğŸ” ê²€ìƒ‰ ì„±ëŠ¥\n")
            
            recall_1 = search_metrics.get('recall@1', 0) * 100
            recall_5 = search_metrics.get('recall@5', 0) * 100
            recall_10 = search_metrics.get('recall@10', 0) * 100
            precision_1 = search_metrics.get('precision@1', 0) * 100
            mrr = search_metrics.get('mrr', 0)
            
            md_content.append(f"- **Recall@1**: {recall_1:.1f}% (ì²« ë²ˆì§¸ ê²°ê³¼ì— ì •ë‹µ í¬í•¨ë¥ )\n")
            md_content.append(f"- **Recall@5**: {recall_5:.1f}% (ìƒìœ„ 5ê°œ ë‚´ ì •ë‹µ í¬í•¨ë¥ )\n")
            md_content.append(f"- **Recall@10**: {recall_10:.1f}% (ìƒìœ„ 10ê°œ ë‚´ ì •ë‹µ í¬í•¨ë¥ )\n")
            md_content.append(f"- **Precision@1**: {precision_1:.1f}% (ì²« ë²ˆì§¸ ê²°ê³¼ì˜ ì •í™•ë„)\n")
            
            # MRRì´ 0ì¸ ê²½ìš° ì²˜ë¦¬
            if mrr > 0:
                avg_rank = 1 / mrr
                md_content.append(f"- **MRR**: {mrr:.3f} (í‰ê·  {avg_rank:.1f}ë²ˆì§¸ ìˆœìœ„ì—ì„œ ì •ë‹µ ë°œê²¬)\n\n")
            else:
                md_content.append(f"- **MRR**: {mrr:.3f} (ì •ë‹µì„ ì°¾ì§€ ëª»í•¨)\n\n")
        
        # ë¶„ì„ ì„±ëŠ¥
        analysis_metrics = results.get('aggregated_metrics', {}).get('analysis_metrics', {})
        if analysis_metrics:
            md_content.append("### ğŸ§  ë¶„ì„ ì„±ëŠ¥\n")
            
            citation_acc = analysis_metrics.get('citation_accuracy', 0) * 100
            sentence_acc = analysis_metrics.get('sentence_prediction_accuracy', 0) * 100
            
            md_content.append(f"- **Citation Accuracy**: {citation_acc:.1f}% (í•„ìˆ˜ íŒë¡€ ì •í™• ì¸ìš©ë¥ )\n")
            md_content.append(f"- **Sentence Prediction**: {sentence_acc:.1f}% (íŒê²° ê²°ê³¼ ì¼ì¹˜ë„)\n")
            
            # ì„ íƒì  ë©”íŠ¸ë¦­
            if 'tag_f1' in analysis_metrics:
                tag_f1 = analysis_metrics['tag_f1'] * 100
                md_content.append(f"- **Tag F1-Score**: {tag_f1:.1f}% (ë²•ë¥  ë¶„ì•¼ íƒœê·¸ ë¶„ë¥˜ ì •í™•ë„)\n")
            
            if 'statute_relevance' in analysis_metrics:
                statute_rel = analysis_metrics['statute_relevance'] * 100
                md_content.append(f"- **Statute Relevance**: {statute_rel:.1f}% (ê´€ë ¨ ë²•ë ¹ ë§¤ì¹­ ì •í™•ë„)\n")
            
            md_content.append("\n")
        
        # ì „ì²´ ì„±ëŠ¥
        if overall:
            md_content.append("### âš¡ ì „ì²´ ì„±ëŠ¥\n")
            
            e2e_acc = overall.get('end_to_end_accuracy', 0) * 100
            avg_latency = overall.get('average_latency_ms', 0)
            success_rate = overall.get('success_rate', 0) * 100
            
            md_content.append(f"- **End-to-End Accuracy**: {e2e_acc:.1f}% (ì™„ì „ ì •ë‹µë¥ )\n")
            md_content.append(f"- **í‰ê·  ì‘ë‹µì‹œê°„**: {avg_latency:,.1f}ms\n")
            md_content.append(f"- **ì„±ê³µë¥ **: {success_rate:.1f}% (API í˜¸ì¶œ ì„±ê³µë¥ )\n\n")
        
        # ê°œì„  ê¶Œì¥ì‚¬í•­
        md_content.append("## ğŸ¯ ê°œì„  ê¶Œì¥ì‚¬í•­\n\n")
        recommendations = self._generate_recommendations(results)
        for i, rec in enumerate(recommendations, 1):
            md_content.append(f"{i}. {rec}\n")
        
        md_content.append("\n")
        
        # ìƒì„¸ ê²°ê³¼ í…Œì´ë¸”
        if config.get('output', {}).get('include_case_details', True):
            md_content.append("## ğŸ“‹ ìƒì„¸ ê²°ê³¼\n\n")
            md_content.append(self._generate_results_table(results.get('case_results', [])))
        
        # ì—ëŸ¬ ì •ë³´
        errors = results.get('errors', [])
        if errors:
            md_content.append("\n## âš ï¸ ì˜¤ë¥˜ ì •ë³´\n\n")
            for error in errors:
                md_content.append(f"- **{error.get('case_id', 'Unknown')}**: {error.get('error', 'Unknown error')}\n")
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(''.join(md_content))
            
            logger.info(f"ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {output_file}")
            
        except Exception as e:
            logger.error(f"ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def _generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ê²€ìƒ‰ ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        search_metrics = results.get('aggregated_metrics', {}).get('search_metrics', {})
        if search_metrics:
            recall_1 = search_metrics.get('recall@1', 0)
            if recall_1 < 0.8:
                recommendations.append(f"ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ í•„ìš” (í˜„ì¬ Recall@1: {recall_1*100:.1f}% â†’ ëª©í‘œ: 80%+)")
        
        # ë¶„ì„ ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        analysis_metrics = results.get('aggregated_metrics', {}).get('analysis_metrics', {})
        if analysis_metrics:
            citation_acc = analysis_metrics.get('citation_accuracy', 0)
            if citation_acc < 0.85:
                recommendations.append(f"íŒë¡€ ì¸ìš© ì •í™•ë„ í–¥ìƒ í•„ìš” (í˜„ì¬: {citation_acc*100:.1f}% â†’ ëª©í‘œ: 85%+)")
            
            sentence_acc = analysis_metrics.get('sentence_prediction_accuracy', 0)
            if sentence_acc < 0.8:
                recommendations.append(f"íŒê²° ì˜ˆì¸¡ ì •í™•ë„ ê°œì„  í•„ìš” (í˜„ì¬: {sentence_acc*100:.1f}% â†’ ëª©í‘œ: 80%+)")
        
        # ì‘ë‹µ ì‹œê°„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        overall = results.get('overall_metrics', {})
        if overall:
            avg_latency = overall.get('average_latency_ms', 0)
            if avg_latency > 2000:
                recommendations.append(f"ì‘ë‹µ ì‹œê°„ ìµœì í™” ê²€í†  (í˜„ì¬: {avg_latency:.0f}ms â†’ ëª©í‘œ: 2ì´ˆ ì´í•˜)")
        
        # ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë¶„ì„ ê¶Œì¥ì‚¬í•­
        case_results = results.get('case_results', [])
        failed_cases = [r for r in case_results if not r.get('analysis_success', True)]
        if failed_cases:
            recommendations.append(f"ì‹¤íŒ¨ ì¼€ì´ìŠ¤ {len(failed_cases)}ê°œ ìƒì„¸ ë¶„ì„ ë° ê°œì„  í•„ìš”")
        
        # ê¸°ë³¸ ê¶Œì¥ì‚¬í•­ (ë©”íŠ¸ë¦­ì´ ì—†ëŠ” ê²½ìš°)
        if not recommendations:
            recommendations.append("ì „ë°˜ì ìœ¼ë¡œ ì–‘í˜¸í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.")
            recommendations.append("ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ì„ í†µí•œ ì„±ëŠ¥ ìœ ì§€ ê¶Œì¥")
        
        return recommendations
    
    def _generate_results_table(self, case_results: List[Dict[str, Any]]) -> str:
        """ì¼€ì´ìŠ¤ë³„ ê²°ê³¼ í…Œì´ë¸” ìƒì„±"""
        if not case_results:
            return "ê²°ê³¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
        
        table = []
        table.append("| ì¼€ì´ìŠ¤ ID | ê²€ìƒ‰ ì„±ê³µ | ë¶„ì„ ì„±ê³µ | ì¸ìš© ë°œê²¬ | íŒê²° ì¼ì¹˜ | ì‘ë‹µì‹œê°„(ms) |\n")
        table.append("|---|---|---|---|---|---:|\n")
        
        for result in case_results[:10]:  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
            case_id = result.get('case_id', 'N/A')
            search_success = "âœ…" if result.get('search_success', False) else "âŒ"
            analysis_success = "âœ…" if result.get('analysis_success', False) else "âŒ"
            citation_found = "âœ…" if result.get('citation_found', False) else "âŒ"
            sentence_match = "âœ…" if result.get('sentence_match', False) else "âŒ"
            latency = result.get('latency_ms', 0)
            
            table.append(f"| {case_id} | {search_success} | {analysis_success} | {citation_found} | {sentence_match} | {latency:.0f} |\n")
        
        if len(case_results) > 10:
            table.append(f"\n*ì´ {len(case_results)}ê°œ ì¼€ì´ìŠ¤ ì¤‘ ìƒìœ„ 10ê°œë§Œ í‘œì‹œ*\n")
        
        return ''.join(table)