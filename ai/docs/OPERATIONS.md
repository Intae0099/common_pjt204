# ğŸ“Š ìš´ì˜ ê°€ì´ë“œ

ALaw AI-Backendì˜ í”„ë¡œë•ì…˜ ìš´ì˜ ë° ëª¨ë‹ˆí„°ë§ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

- [ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§](#ì‹œìŠ¤í…œ-ëª¨ë‹ˆí„°ë§)
- [ë¡œê·¸ ê´€ë¦¬](#ë¡œê·¸-ê´€ë¦¬)
- [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)
- [ì¥ì•  ëŒ€ì‘](#ì¥ì• -ëŒ€ì‘)
- [ë°±ì—… ë° ë³µêµ¬](#ë°±ì—…-ë°-ë³µêµ¬)
- [ë³´ì•ˆ ê´€ë¦¬](#ë³´ì•ˆ-ê´€ë¦¬)
- [ìš©ëŸ‰ ê³„íš](#ìš©ëŸ‰-ê³„íš)

---

## ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§

### 1. í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸

#### ê¸°ë³¸ í—¬ìŠ¤ì²´í¬
```bash
# ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒíƒœ í™•ì¸
curl http://localhost:8997/

# í ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬
curl http://localhost:8997/api/queue/health

# ìƒì„¸ í ìƒíƒœ (JSON)
curl http://localhost:8997/api/queue/status/json
```

#### ì‘ë‹µ ì˜ˆì‹œ
```json
{
    "status": "healthy",
    "is_running": true,
    "queue_stats": {
        "search": {"pending": 0, "processing": 1, "completed": 15},
        "analysis": {"pending": 2, "processing": 1, "completed": 8}
    },
    "resource_usage": {
        "memory_percent": 68.5,
        "cpu_percent": 35.2
    },
    "timestamp": "2024-01-01T12:00:00Z"
}
```

### 2. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§

#### CPU ë° ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
```bash
# Docker ì»¨í…Œì´ë„ˆ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
docker stats alaw-ai-backend

# ì‹œìŠ¤í…œ ì „ì²´ ë¦¬ì†ŒìŠ¤
htop

# íŠ¹ì • í”„ë¡œì„¸ìŠ¤ ëª¨ë‹ˆí„°ë§
ps aux | grep python
```

#### ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰
```bash
# ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸
df -h

# ë¡œê·¸ íŒŒì¼ í¬ê¸° í™•ì¸
du -sh logs/
find logs/ -name "*.log" -size +100M

# ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° í™•ì¸
docker exec postgres psql -U postgres -c "SELECT pg_size_pretty(pg_database_size('alaw_ai'));"
```

### 3. ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­

#### ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
```python
# metrics.py
import time
import psutil
from prometheus_client import Counter, Histogram, Gauge

# ìš”ì²­ ì¹´ìš´í„°
REQUEST_COUNT = Counter('app_requests_total', 'Total requests', ['method', 'endpoint'])

# ì‘ë‹µ ì‹œê°„ íˆìŠ¤í† ê·¸ë¨
REQUEST_DURATION = Histogram('app_request_duration_seconds', 'Request duration')

# ë¦¬ì†ŒìŠ¤ ê²Œì´ì§€
MEMORY_USAGE = Gauge('app_memory_usage_bytes', 'Memory usage')
CPU_USAGE = Gauge('app_cpu_usage_percent', 'CPU usage')

def update_system_metrics():
    """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
    process = psutil.Process()
    MEMORY_USAGE.set(process.memory_info().rss)
    CPU_USAGE.set(process.cpu_percent())
```

### 4. ì•Œë¦¼ ì„¤ì •

#### Mattermost ì•Œë¦¼ (ê¸°ì¡´)
```bash
# ì¥ì•  ì•Œë¦¼ ìŠ¤í¬ë¦½íŠ¸
#!/bin/bash
# alert.sh

WEBHOOK_URL="your-mattermost-webhook-url"
SERVICE_URL="http://localhost:8997"

if ! curl -f -s "$SERVICE_URL/api/queue/health" > /dev/null; then
    curl -X POST "$WEBHOOK_URL" \
        -H 'Content-Type: application/json' \
        -d '{
            "text": "ğŸš¨ ALaw AI-Backend ì„œë¹„ìŠ¤ ì¥ì•  ê°ì§€",
            "channel": "alerts",
            "username": "monitoring-bot"
        }'
fi
```

#### ì´ë©”ì¼ ì•Œë¦¼
```python
# email_alert.py
import smtplib
from email.mime.text import MIMEText

def send_alert_email(subject: str, message: str):
    """ì•Œë¦¼ ì´ë©”ì¼ ë°œì†¡"""
    msg = MIMEText(message)
    msg['Subject'] = f"[ALaw AI] {subject}"
    msg['From'] = "alerts@alaw.ai"
    msg['To'] = "admin@alaw.ai"
    
    server = smtplib.SMTP('smtp.gmail.com', 587)
    server.starttls()
    server.login("alerts@alaw.ai", "app_password")
    server.send_message(msg)
    server.quit()

# ì‚¬ìš© ì˜ˆì‹œ
def check_and_alert():
    try:
        response = requests.get("http://localhost:8997/api/queue/health")
        if response.status_code != 200:
            send_alert_email(
                "ì„œë¹„ìŠ¤ ì¥ì• ",
                f"í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: HTTP {response.status_code}"
            )
    except Exception as e:
        send_alert_email("ì„œë¹„ìŠ¤ ì¥ì• ", f"í—¬ìŠ¤ì²´í¬ ì˜¤ë¥˜: {str(e)}")
```

---

## ë¡œê·¸ ê´€ë¦¬

### 1. ë¡œê·¸ êµ¬ì¡°

```
logs/
â”œâ”€â”€ app.log              # ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê·¸
â”œâ”€â”€ access.log           # HTTP ìš”ì²­ ë¡œê·¸
â”œâ”€â”€ error.log            # ì—ëŸ¬ ì „ìš© ë¡œê·¸
â”œâ”€â”€ performance.log      # ì„±ëŠ¥ ê´€ë ¨ ë¡œê·¸
â””â”€â”€ archive/             # ì•„ì¹´ì´ë¸Œëœ ë¡œê·¸
    â”œâ”€â”€ app.log.2024-01-01.gz
    â””â”€â”€ app.log.2024-01-02.gz
```

### 2. ë¡œê·¸ ë ˆë²¨ ê´€ë¦¬

#### í”„ë¡œë•ì…˜ ë¡œê·¸ ì„¤ì •
```python
# config/logging.py
LOGGING_CONFIG = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'detailed': {
            'format': '%(asctime)s [%(levelname)s] %(name)s: %(message)s'
        },
        'json': {
            'format': '{"timestamp": "%(asctime)s", "level": "%(levelname)s", "logger": "%(name)s", "message": "%(message)s"}'
        }
    },
    'handlers': {
        'file': {
            'class': 'logging.handlers.RotatingFileHandler',
            'filename': 'logs/app.log',
            'maxBytes': 50 * 1024 * 1024,  # 50MB
            'backupCount': 5,
            'formatter': 'detailed'
        },
        'error_file': {
            'class': 'logging.handlers.RotatingFileHandler',
            'filename': 'logs/error.log',
            'maxBytes': 10 * 1024 * 1024,  # 10MB
            'backupCount': 10,
            'formatter': 'json',
            'level': 'ERROR'
        }
    },
    'loggers': {
        'root': {
            'level': 'INFO',
            'handlers': ['file', 'error_file']
        }
    }
}
```

### 3. ë¡œê·¸ ë¶„ì„

#### ìì£¼ ì‚¬ìš©í•˜ëŠ” ë¡œê·¸ ë¶„ì„ ëª…ë ¹ì–´
```bash
# ì—ëŸ¬ ë¡œê·¸ í™•ì¸
grep "ERROR" logs/app.log | tail -20

# íŠ¹ì • ì‹œê°„ëŒ€ ë¡œê·¸ í•„í„°ë§
grep "2024-01-01 12:" logs/app.log

# API ì‘ë‹µ ì‹œê°„ ë¶„ì„
grep "response_time" logs/app.log | awk '{print $NF}' | sort -n | tail -10

# ê°€ì¥ ë¹ˆë²ˆí•œ ì—ëŸ¬ TOP 10
grep "ERROR" logs/app.log | cut -d':' -f4- | sort | uniq -c | sort -nr | head -10

# ì‹œê°„ë‹¹ ìš”ì²­ ìˆ˜ ë¶„ì„
grep "POST\|GET" logs/access.log | cut -d' ' -f1 | cut -d':' -f1-2 | uniq -c
```

#### ë¡œê·¸ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
```python
# log_analyzer.py
import re
import json
from datetime import datetime, timedelta

def analyze_error_patterns(log_file: str, hours: int = 24) -> dict:
    """ì§€ë‚œ Nì‹œê°„ ë™ì•ˆì˜ ì—ëŸ¬ íŒ¨í„´ ë¶„ì„"""
    cutoff_time = datetime.now() - timedelta(hours=hours)
    error_patterns = {}
    
    with open(log_file, 'r') as f:
        for line in f:
            if 'ERROR' in line:
                # íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹±
                timestamp_match = re.search(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', line)
                if timestamp_match:
                    timestamp = datetime.strptime(timestamp_match.group(1), '%Y-%m-%d %H:%M:%S')
                    if timestamp >= cutoff_time:
                        # ì—ëŸ¬ ë©”ì‹œì§€ ì¶”ì¶œ
                        error_msg = line.split('ERROR')[-1].strip()
                        error_patterns[error_msg] = error_patterns.get(error_msg, 0) + 1
    
    return dict(sorted(error_patterns.items(), key=lambda x: x[1], reverse=True))

def generate_log_report():
    """ë¡œê·¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
    errors = analyze_error_patterns('logs/error.log')
    
    report = {
        'timestamp': datetime.now().isoformat(),
        'top_errors': list(errors.items())[:10],
        'total_errors': sum(errors.values()),
        'unique_errors': len(errors)
    }
    
    return report
```

### 4. ë¡œê·¸ ë¡œí…Œì´ì…˜

#### logrotate ì„¤ì •
```bash
# /etc/logrotate.d/alaw-ai
/path/to/alaw-ai/logs/*.log {
    daily
    rotate 30
    compress
    delaycompress
    missingok
    notifempty
    create 0644 app app
    postrotate
        docker exec alaw-ai-backend kill -HUP 1
    endscript
}
```

---

## ì„±ëŠ¥ ìµœì í™”

### 1. ì‘ë‹µ ì‹œê°„ ìµœì í™”

#### ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™”
```sql
-- ì¸ë±ìŠ¤ ì‚¬ìš©ë¥  í™•ì¸
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes 
ORDER BY idx_tup_read DESC;

-- ëŠë¦° ì¿¼ë¦¬ ì‹ë³„
SELECT 
    query,
    calls,
    total_time,
    mean_time,
    rows
FROM pg_stat_statements 
ORDER BY mean_time DESC 
LIMIT 10;
```

#### ì• í”Œë¦¬ì¼€ì´ì…˜ ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§
```python
# profiling.py
import cProfile
import pstats
from functools import wraps

def profile_function(func):
    """í•¨ìˆ˜ ì‹¤í–‰ í”„ë¡œíŒŒì¼ë§"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        profiler = cProfile.Profile()
        profiler.enable()
        
        try:
            result = func(*args, **kwargs)
            return result
        finally:
            profiler.disable()
            stats = pstats.Stats(profiler)
            stats.sort_stats('cumulative')
            stats.print_stats(10)  # ìƒìœ„ 10ê°œ í•¨ìˆ˜
    
    return wrapper

# ì‚¬ìš© ì˜ˆì‹œ
@profile_function
async def slow_function():
    # ë¶„ì„í•  í•¨ìˆ˜
    pass
```

### 2. ë©”ëª¨ë¦¬ ìµœì í™”

#### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
```python
# memory_monitor.py
import psutil
import gc
from memory_profiler import profile

@profile
def memory_intensive_function():
    """ë©”ëª¨ë¦¬ ì§‘ì•½ì  í•¨ìˆ˜ ë¶„ì„"""
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„í•  ì½”ë“œ
    pass

def check_memory_leaks():
    """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì²´í¬"""
    gc.collect()
    memory_usage = psutil.Process().memory_info().rss / 1024 / 1024  # MB
    
    if memory_usage > 1000:  # 1GB ì´ˆê³¼ ì‹œ ì•Œë¦¼
        logger.warning(f"ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì§€: {memory_usage:.2f}MB")
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìƒíƒœ í™•ì¸
        logger.info(f"GC ì¹´ìš´íŠ¸: {gc.get_count()}")
        logger.info(f"GC í†µê³„: {gc.get_stats()}")
```

### 3. í ì‹œìŠ¤í…œ ìµœì í™”

#### í ì„±ëŠ¥ íŠœë‹
```python
# queue_tuning.py
OPTIMIZED_LIMITS = {
    "search": {
        "max_concurrent": 3,  # ê¸°ë³¸ê°’: 2
        "max_queue_size": 20,  # ê¸°ë³¸ê°’: 10
        "timeout": 60  # ê¸°ë³¸ê°’: 30
    },
    "case_analysis": {
        "max_concurrent": 2,  # ê¸°ë³¸ê°’: 1
        "max_queue_size": 15,  # ê¸°ë³¸ê°’: 5
        "timeout": 120  # ê¸°ë³¸ê°’: 60
    }
}

def optimize_queue_settings():
    """ë¦¬ì†ŒìŠ¤ ìƒí™©ì— ë”°ë¥¸ í ì„¤ì • ìµœì í™”"""
    memory_percent = psutil.virtual_memory().percent
    cpu_percent = psutil.cpu_percent(interval=1)
    
    if memory_percent < 60 and cpu_percent < 50:
        # ë¦¬ì†ŒìŠ¤ ì—¬ìœ  ìˆì„ ë•Œ ë™ì‹œ ì²˜ë¦¬ ì¦ê°€
        return OPTIMIZED_LIMITS
    else:
        # ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ì‹œ ë³´ìˆ˜ì  ì„¤ì •
        return CONSERVATIVE_LIMITS
```

---

## ì¥ì•  ëŒ€ì‘

### 1. ì¼ë°˜ì ì¸ ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤

#### ì„œë¹„ìŠ¤ ì‘ë‹µ ì—†ìŒ
```bash
# 1. í”„ë¡œì„¸ìŠ¤ ìƒíƒœ í™•ì¸
docker ps | grep alaw-ai
docker logs alaw-ai-backend --tail=50

# 2. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸
docker stats alaw-ai-backend

# 3. í—¬ìŠ¤ì²´í¬ ì§ì ‘ ì‹¤í–‰
curl -v http://localhost:8997/api/queue/health

# 4. ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘
docker restart alaw-ai-backend

# 5. ì™„ì „ ì¬ë°°í¬ (í•„ìš”ì‹œ)
cd docker && docker-compose down && docker-compose up -d --build
```

#### ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨
```bash
# 1. PostgreSQL ìƒíƒœ í™•ì¸
docker exec postgres pg_isready

# 2. ì—°ê²° í…ŒìŠ¤íŠ¸
docker exec postgres psql -U postgres -c "SELECT 1;"

# 3. ì—°ê²° ìˆ˜ í™•ì¸
docker exec postgres psql -U postgres -c "SELECT count(*) FROM pg_stat_activity;"

# 4. ë°ì´í„°ë² ì´ìŠ¤ ì¬ì‹œì‘ (í•„ìš”ì‹œ)
cd db && docker-compose restart postgres
```

#### í ì‹œìŠ¤í…œ ì •ì²´
```bash
# 1. í ìƒíƒœ í™•ì¸
curl http://localhost:8997/api/queue/status/json

# 2. ì •ì²´ëœ ì‘ì—… í™•ì¸
sqlite3 data/queue.db "SELECT * FROM tasks WHERE status='processing' ORDER BY created_at;"

# 3. í ì •ë¦¬ (ì‹ ì¤‘íˆ ì‹¤í–‰)
curl -X POST http://localhost:8997/api/queue/clear

# 4. ì„œë¹„ìŠ¤ ì¬ì‹œì‘
docker restart alaw-ai-backend
```

### 2. ìë™ ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# auto_recovery.sh

LOG_FILE="/var/log/alaw-ai-recovery.log"
SERVICE_URL="http://localhost:8997"

log_message() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" >> "$LOG_FILE"
}

check_and_recover() {
    log_message "í—¬ìŠ¤ì²´í¬ ì‹œì‘"
    
    # í—¬ìŠ¤ì²´í¬ ì‹¤í–‰
    if curl -f -s "$SERVICE_URL/api/queue/health" > /dev/null; then
        log_message "ì„œë¹„ìŠ¤ ì •ìƒ"
        return 0
    fi
    
    log_message "ì„œë¹„ìŠ¤ ì¥ì•  ê°ì§€, ë³µêµ¬ ì‹œì‘"
    
    # 1ì°¨ ì‹œë„: ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘
    docker restart alaw-ai-backend
    sleep 30
    
    if curl -f -s "$SERVICE_URL/api/queue/health" > /dev/null; then
        log_message "ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘ìœ¼ë¡œ ë³µêµ¬ ì™„ë£Œ"
        return 0
    fi
    
    # 2ì°¨ ì‹œë„: ì™„ì „ ì¬ë°°í¬
    cd /path/to/docker
    docker-compose down
    docker-compose up -d --build
    sleep 60
    
    if curl -f -s "$SERVICE_URL/api/queue/health" > /dev/null; then
        log_message "ì™„ì „ ì¬ë°°í¬ë¡œ ë³µêµ¬ ì™„ë£Œ"
        return 0
    fi
    
    log_message "ìë™ ë³µêµ¬ ì‹¤íŒ¨, ìˆ˜ë™ ê°œì… í•„ìš”"
    # ì•Œë¦¼ ë°œì†¡
    send_alert_notification "ìë™ ë³µêµ¬ ì‹¤íŒ¨"
    
    return 1
}

# ë§¤ 5ë¶„ë§ˆë‹¤ ì‹¤í–‰í•˜ë„ë¡ crontab ì„¤ì •
# */5 * * * * /path/to/auto_recovery.sh
```

### 3. ì¥ì•  ì—ìŠ¤ì»¬ë ˆì´ì…˜

#### 1ë‹¨ê³„: ìë™ ë³µêµ¬
- í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨ ì‹œ ìë™ ì¬ì‹œì‘
- í ì •ì²´ ì‹œ ìë™ ì •ë¦¬
- ì„ì‹œ íŒŒì¼ ì •ë¦¬

#### 2ë‹¨ê³„: ê°œë°œíŒ€ ì•Œë¦¼
- Mattermost/Slack ì•Œë¦¼
- ì´ë©”ì¼ ì•Œë¦¼
- SMS ì•Œë¦¼ (ì¤‘ìš” ì¥ì• )

#### 3ë‹¨ê³„: ìˆ˜ë™ ê°œì…
- ë¡œê·¸ ë¶„ì„
- ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸
- ì¸í”„ë¼ ì ê²€

---

## ë°±ì—… ë° ë³µêµ¬

### 1. ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…

#### ìë™ ë°±ì—… ìŠ¤í¬ë¦½íŠ¸
```bash
#!/bin/bash
# backup_db.sh

BACKUP_DIR="/backups/postgres"
DATE=$(date +%Y%m%d_%H%M%S)
DB_NAME="alaw_ai"

# ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p "$BACKUP_DIR"

# ë°ì´í„°ë² ì´ìŠ¤ ë¤í”„
docker exec postgres pg_dump -U postgres "$DB_NAME" | gzip > "$BACKUP_DIR/backup_${DATE}.sql.gz"

# ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì •ë¦¬ (30ì¼ ì´ìƒ)
find "$BACKUP_DIR" -name "backup_*.sql.gz" -mtime +30 -delete

echo "ë°±ì—… ì™„ë£Œ: backup_${DATE}.sql.gz"
```

#### ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸
```bash
#!/bin/bash
# restore_db.sh

BACKUP_FILE="$1"
DB_NAME="alaw_ai"

if [ -z "$BACKUP_FILE" ]; then
    echo "ì‚¬ìš©ë²•: $0 <ë°±ì—…íŒŒì¼ê²½ë¡œ>"
    exit 1
fi

# ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ì‚­ì œ ë° ì¬ìƒì„± (ì£¼ì˜!)
docker exec postgres dropdb -U postgres "$DB_NAME"
docker exec postgres createdb -U postgres "$DB_NAME"

# ë°±ì—… ë³µì›
gunzip -c "$BACKUP_FILE" | docker exec -i postgres psql -U postgres "$DB_NAME"

echo "ë³µêµ¬ ì™„ë£Œ"
```

### 2. ì„¤ì • íŒŒì¼ ë°±ì—…

```bash
#!/bin/bash
# backup_config.sh

BACKUP_DIR="/backups/config"
DATE=$(date +%Y%m%d_%H%M%S)

mkdir -p "$BACKUP_DIR"

# ì„¤ì • íŒŒì¼ë“¤ ë°±ì—…
tar -czf "$BACKUP_DIR/config_${DATE}.tar.gz" \
    config/ \
    docker/ \
    logs/ \
    --exclude="logs/*.log" \
    --exclude="*.pyc"

echo "ì„¤ì • ë°±ì—… ì™„ë£Œ: config_${DATE}.tar.gz"
```

### 3. BM25 ê²€ìƒ‰ ìºì‹œ ê´€ë¦¬

- **ìºì‹œ íŒŒì¼**: `data/preprocessed/bm25_cache.pkl`
- **ì—­í• **: BM25 ê²€ìƒ‰ ëª¨ë¸ì„ ì €ì¥í•˜ì—¬ ì„œë²„ ì‹œì‘ ì‹œê°„ì„ ë‹¨ì¶•ì‹œí‚¤ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
- **ìš´ì˜ ê°€ì´ë“œ**:
  - **ë°±ì—…**: ì´ íŒŒì¼ì€ ì›ë³¸ ë°ì´í„°ë¡œë¶€í„° ì¬ìƒì„± ê°€ëŠ¥í•˜ë¯€ë¡œ, í•„ìˆ˜ ë°±ì—… ëŒ€ìƒì€ ì•„ë‹™ë‹ˆë‹¤. í•˜ì§€ë§Œ ìš©ëŸ‰ì´ í¬ê³  ìƒì„±ì— ìˆ˜ ë¶„ì´ ê±¸ë¦¬ë¯€ë¡œ, ì„œë²„ ì´ì „ ë“± ë¹ ë¥¸ ë³µêµ¬ê°€ í•„ìš”í•  ê²½ìš° í•¨ê»˜ ë°±ì—…í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
  - **ê°±ì‹ **: ìƒˆë¡œìš´ íŒë¡€ ë°ì´í„°ê°€ `data/preprocessed`ì— ì¶”ê°€ë˜ì–´ ê²€ìƒ‰ ëª¨ë¸ì„ ê°±ì‹ í•´ì•¼ í•  ê²½ìš°, ì´ ìºì‹œ íŒŒì¼ì„ ì‚­ì œ í›„ ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ë©´ ìë™ìœ¼ë¡œ ì¬ìƒì„±ë©ë‹ˆë‹¤.

### 3. í¬ë¡  ì‘ì—… ì„¤ì •
```

```bash
# crontab -e
# ë§¤ì¼ ì˜¤ì „ 2ì‹œ ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…
0 2 * * * /path/to/backup_db.sh

# ë§¤ì£¼ ì¼ìš”ì¼ ì˜¤ì „ 3ì‹œ ì„¤ì • ë°±ì—…
0 3 * * 0 /path/to/backup_config.sh

# ë§¤ 5ë¶„ë§ˆë‹¤ í—¬ìŠ¤ì²´í¬ ë° ìë™ ë³µêµ¬
*/5 * * * * /path/to/auto_recovery.sh
```

---

## ë³´ì•ˆ ê´€ë¦¬

### 1. ì ‘ê·¼ ì œì–´

#### ë°©í™”ë²½ ì„¤ì •
```bash
# UFW ì„¤ì • (Ubuntu)
sudo ufw allow 22          # SSH
sudo ufw allow 8997        # ì• í”Œë¦¬ì¼€ì´ì…˜
sudo ufw allow 5432/tcp    # PostgreSQL (í•„ìš”ì‹œ)
sudo ufw enable
```

#### Nginx í”„ë¡ì‹œ ë³´ì•ˆ
```nginx
# /etc/nginx/sites-available/alaw-ai
server {
    listen 80;
    server_name alaw-ai.example.com;
    
    # ë³´ì•ˆ í—¤ë”
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    
    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    
    location /api/ {
        limit_req zone=api burst=20 nodelay;
        proxy_pass http://localhost:8997;
        
        # í´ë¼ì´ì–¸íŠ¸ IP ì „ë‹¬
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }
}
```

### 2. ë³´ì•ˆ ëª¨ë‹ˆí„°ë§

#### ì ‘ê·¼ ë¡œê·¸ ë¶„ì„
```bash
# ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì ‘ê·¼ íŒ¨í„´ íƒì§€
tail -f /var/log/nginx/access.log | grep -E "(POST|PUT|DELETE)" | grep -v "200\|201"

# IPë³„ ìš”ì²­ ë¹ˆë„ ë¶„ì„
awk '{print $1}' /var/log/nginx/access.log | sort | uniq -c | sort -nr | head -20

# ì‹¤íŒ¨í•œ ë¡œê·¸ì¸ ì‹œë„ ê°ì§€
grep "401\|403" /var/log/nginx/access.log | tail -20
```

### 3. ë¯¼ê° ì •ë³´ ë³´í˜¸

#### í™˜ê²½ ë³€ìˆ˜ ì•”í˜¸í™”
```bash
# ë¯¼ê°í•œ í™˜ê²½ ë³€ìˆ˜ëŠ” ì•”í˜¸í™”í•˜ì—¬ ì €ì¥
echo "sensitive_value" | gpg --symmetric --armor > .env.encrypted

# ì‹¤í–‰ ì‹œ ë³µí˜¸í™”
gpg --decrypt .env.encrypted > .env.tmp
source .env.tmp
rm .env.tmp
```

---

## ìš©ëŸ‰ ê³„íš

### 1. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡

#### ë™ì‹œ ì‚¬ìš©ì ê¸°ì¤€ ìš©ëŸ‰ ê³„íš
```python
# capacity_planning.py
def calculate_resources(concurrent_users: int) -> dict:
    """ë™ì‹œ ì‚¬ìš©ì ìˆ˜ ê¸°ë°˜ ë¦¬ì†ŒìŠ¤ ê³„ì‚°"""
    
    # ì‚¬ìš©ìë‹¹ í‰ê·  ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
    cpu_per_user = 0.1  # CPU ì½”ì–´
    memory_per_user = 50  # MB
    
    # ê¸°ë³¸ ì‹œìŠ¤í…œ ì˜¤ë²„í—¤ë“œ
    base_cpu = 2
    base_memory = 2048  # MB
    
    total_cpu = base_cpu + (concurrent_users * cpu_per_user)
    total_memory = base_memory + (concurrent_users * memory_per_user)
    
    return {
        "concurrent_users": concurrent_users,
        "required_cpu_cores": round(total_cpu, 1),
        "required_memory_mb": int(total_memory),
        "recommended_cpu_cores": round(total_cpu * 1.5, 1),  # 50% ì—¬ìœ 
        "recommended_memory_mb": int(total_memory * 1.3)      # 30% ì—¬ìœ 
    }

# ì˜ˆì¸¡ ê²°ê³¼
for users in [10, 50, 100, 200]:
    resources = calculate_resources(users)
    print(f"ë™ì‹œ ì‚¬ìš©ì {users}ëª…: CPU {resources['recommended_cpu_cores']}ì½”ì–´, "
          f"ë©”ëª¨ë¦¬ {resources['recommended_memory_mb']}MB")
```

### 2. í™•ì¥ ê³„íš

#### ìˆ˜ì§ í™•ì¥ (Scale Up)
```yaml
# docker-compose.scale-up.yml
version: '3.8'
services:
  ai-app:
    deploy:
      resources:
        limits:
          cpus: '8.0'
          memory: 16G
        reservations:
          cpus: '4.0'
          memory: 8G
```

#### ìˆ˜í‰ í™•ì¥ (Scale Out)
```yaml
# docker-compose.scale-out.yml
version: '3.8'
services:
  ai-app:
    scale: 3  # 3ê°œ ì¸ìŠ¤í„´ìŠ¤ ì‹¤í–‰
    environment:
      - INSTANCE_ID=${HOSTNAME}
  
  load-balancer:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
```

---

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

- [ë°°í¬ ê°€ì´ë“œ](DEPLOYMENT.md)
- [ì•„í‚¤í…ì²˜ ê°€ì´ë“œ](ARCHITECTURE.md)
- [API ë¬¸ì„œ](API.md)
- [ê°œë°œ ê°€ì´ë“œ](DEVELOPMENT.md)