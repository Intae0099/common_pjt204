import psycopg2
from pgvector.psycopg2 import register_vector
import os
from dotenv import load_dotenv
from datetime import date

from db.database import get_psycopg2_connection
from utils.logger import setup_logger, get_logger
from llm.models.embedding_model import EmbeddingModel
from llm.models.cross_encoder_model import CrossEncoderModel

load_dotenv()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë¡œê±° ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
setup_logger()
logger = get_logger(__name__)

class SearchService:
    def __init__(self, embedding_model: EmbeddingModel, cross_encoder_model: CrossEncoderModel):
        self.embedding_model = embedding_model
        self.cross_encoder_model = cross_encoder_model

    async def vector_search(self, query: str, page: int = 1, size: int = 10, use_rerank: bool = True) -> tuple[list[dict], int]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ìš°ì„ ìœ¼ë¡œ í•˜ê³  ë²¡í„° ê²€ìƒ‰ì„ ë³´ì¡°ë¡œ ì‚¬ìš©í•œë‹¤.
        ë²¡í„° ê²€ìƒ‰ ì„±ëŠ¥ì´ ë‚®ì•„ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì„ ë©”ì¸ìœ¼ë¡œ ì „í™˜.
        """
        conn = None
        try:
            conn = get_psycopg2_connection()
            register_vector(conn)

            with conn.cursor() as cur:
                # ì „ì²´ ê°œìˆ˜ (í‚¤ì›Œë“œ ê¸°ë°˜)
                cur.execute("""
                    SELECT COUNT(DISTINCT lc.case_id) 
                    FROM legal_cases lc 
                    WHERE lc.title ILIKE %s OR lc.full_text ILIKE %s OR lc.category ILIKE %s
                """, (f'%{query}%', f'%{query}%', f'%{query}%'))
                keyword_total = cur.fetchone()[0]

                # 1ë‹¨ê³„: í‚¤ì›Œë“œ ê²€ìƒ‰ (ì£¼ë ¥)
                keyword_size = min(size * 2, 30)  # ìš”ì²­ì˜ 2ë°° ë˜ëŠ” ìµœëŒ€ 30ê°œ
                cur.execute("""
                    SELECT lc.case_id, lc.title, lc.decision_date, lc.category, 
                           lc.issue, lc.summary, lc.full_text, lc.statutes,
                           '' as chunk_text,
                           CASE 
                               WHEN lc.title ILIKE %s THEN 3
                               WHEN lc.category ILIKE %s THEN 2  
                               ELSE 1
                           END as keyword_relevance
                    FROM legal_cases lc
                    WHERE lc.title ILIKE %s OR lc.full_text ILIKE %s OR lc.category ILIKE %s
                    ORDER BY keyword_relevance DESC, lc.decision_date DESC
                    LIMIT %s
                    """, (f'%{query}%', f'%{query}%', f'%{query}%', f'%{query}%', f'%{query}%', keyword_size))
                
                keyword_results = [
                    {
                        "case_id": cid,
                        "title": title,
                        "decision_date": decision_date,
                        "category": category,
                        "issue": issue,
                        "summary": summary,
                        "full_text": full_text,
                        "chunk_text": chunk_text,
                        "statutes": statutes,
                        "_source": "keyword"
                    }
                    for cid, title, decision_date, category, issue, summary, full_text, statutes, chunk_text, relevance in cur.fetchall()
                ]

                # 2ë‹¨ê³„: í‚¤ì›Œë“œ ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ë³´ì™„
                initial_results = keyword_results
                if len(keyword_results) < size:
                    logger.info(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ë¶€ì¡± ({len(keyword_results)}ê°œ), ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ë³´ì™„...")
                    
                    query_embedding = self.embedding_model.get_embedding(query)
                    
                    # í‚¤ì›Œë“œ ê²€ìƒ‰ì—ì„œ ë‚˜ì˜¨ case_idë“¤ ì œì™¸
                    exclude_ids = [r["case_id"] for r in keyword_results] if keyword_results else []
                    exclude_clause = ""
                    exclude_params = []
                    
                    if exclude_ids:
                        placeholders = ",".join(["%s"] * len(exclude_ids))
                        exclude_clause = f"AND lc.case_id NOT IN ({placeholders})"
                        exclude_params = exclude_ids
                    
                    vector_size = size - len(keyword_results)
                    vector_query = f"""
                        SELECT lc.case_id, lc.title, lc.decision_date, lc.category, 
                               lc.issue, lc.summary, lc.full_text, lch.chunk_text, lc.statutes
                        FROM legal_chunks lch
                        JOIN legal_cases lc ON lch.case_id = lc.case_id
                        WHERE 1=1 {exclude_clause}
                        ORDER BY lch.embedding <-> %s::vector
                        LIMIT %s
                    """
                    
                    cur.execute(vector_query, exclude_params + [query_embedding, vector_size])
                    
                    vector_results = [
                        {
                            "case_id": cid,
                            "title": title,
                            "decision_date": decision_date,
                            "category": category,
                            "issue": issue,
                            "summary": summary,
                            "full_text": full_text,
                            "chunk_text": chunk_text,
                            "statutes": statutes,
                            "_source": "vector"
                        }
                        for cid, title, decision_date, category, issue, summary, full_text, chunk_text, statutes in cur.fetchall()
                    ]
                    
                    initial_results.extend(vector_results)
                
                # ì „ì²´ countëŠ” í‚¤ì›Œë“œ ê¸°ë°˜ + ë²¡í„° ì „ì²´ë¥¼ í•©ì‚° ì¶”ì •
                if keyword_total > 0:
                    total_count = keyword_total
                else:
                    cur.execute("SELECT COUNT(DISTINCT lc.case_id) FROM legal_chunks lch JOIN legal_cases lc ON lch.case_id = lc.case_id")
                    total_count = cur.fetchone()[0]

            # ğŸ”§ DEBUG: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
            keyword_count = len([r for r in initial_results if r.get("_source") == "keyword"])
            vector_count = len([r for r in initial_results if r.get("_source") == "vector"])
            logger.info(f"[hybrid_search] total={len(initial_results)} (keyword={keyword_count}, vector={vector_count})")

            if use_rerank and len(initial_results) > 1:
                logger.info("Applying reranking to hybrid search results...")
                reranked = self._rerank_cases(query, initial_results, requested_size=size)

                # ë””ë²„ê·¸: ìµœì¢… ê²°ê³¼ ì†ŒìŠ¤ ë¶„í¬
                final_keyword = len([r for r in reranked if r.get("_source") == "keyword"])
                final_vector = len([r for r in reranked if r.get("_source") == "vector"])
                logger.info(f"[final_results] total={len(reranked)} (keyword={final_keyword}, vector={final_vector})")

                return reranked, total_count
            else:
                logger.info("Reranking skipped or insufficient results.")
                # ìš”ì²­ëœ ê°œìˆ˜ë§Œí¼ë§Œ ë°˜í™˜
                return initial_results[:size], total_count

        except psycopg2.Error as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {e}")
            return [], 0
        except Exception as e:
            logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return [], 0
        finally:
            if conn:
                conn.close()

    async def get_case_by_id(self, prec_id: str) -> dict | None:
        """
        íŒë¡€ IDë¡œ íŒë¡€ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
        """
        conn = None
        try:
            conn = get_psycopg2_connection()
            with conn.cursor() as cur:
                cur.execute(
                    """
                    SELECT case_id, title, decision_date, category, issue, summary, statutes, precedents, full_text
                    FROM legal_cases
                    WHERE case_id = %s
                    """,
                    (prec_id,)
                )
                result = cur.fetchone()
                if result:
                    columns = [desc[0] for desc in cur.description]
                    return dict(zip(columns, result))
                return None
        except psycopg2.Error as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {e}")
            return None
        except Exception as e:
            logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return None
        finally:
            if conn:
                conn.close()

    def _rerank_cases(self, query: str, initial_results: list[dict], requested_size: int = 10) -> list[dict]:
        """
        Cross-encoder ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¬í‰ê°€í•˜ê³  í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ë¶€ìŠ¤íŒ…í•˜ì—¬ ê´€ë ¨ë„ ìˆœìœ¼ë¡œ ì¬ì •ë ¬í•œë‹¤.
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            initial_results: ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼
            requested_size: ì‚¬ìš©ìê°€ ìš”ì²­í•œ ê²°ê³¼ ê°œìˆ˜
            
        Returns:
            ì¬ì •ë ¬ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (requested_size ë§Œí¼)
        """
        if not initial_results:
            return []

        # 1. Cross-encoder ì ìˆ˜ ê³„ì‚°
        documents_to_rerank = []
        for i, doc in enumerate(initial_results):
            # ì œëª© + ì¹´í…Œê³ ë¦¬ + chunk_textë¥¼ ì¡°í•©í•˜ì—¬ ë” í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
            title = doc.get('title', '') or ''
            category = doc.get('category', '') or ''
            chunk_text = doc.get('chunk_text', '') or ''
            issue = doc.get('issue', '') or ''
            
            # ì œëª©ê³¼ ì¹´í…Œê³ ë¦¬ë¥¼ ìš°ì„ ì‹œí•˜ê³  chunk_textë¥¼ ë³´ì¡°ë¡œ ì‚¬ìš©
            combined_text = f"{title} {category} {issue} {chunk_text}".strip()
            documents_to_rerank.append(combined_text)

        scores = self.cross_encoder_model.get_cross_encoder_scores(query, documents_to_rerank)

        # 2. í‚¤ì›Œë“œ ë§¤ì¹­ ë¶€ìŠ¤íŒ… ì ìš©
        query_keywords = query.lower().split()
        scored_results = []
        
        for i, doc in enumerate(initial_results):
            base_score = scores[i]
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤ ê³„ì‚°
            title = (doc.get('title', '') or '').lower()
            category = (doc.get('category', '') or '').lower()
            
            keyword_boost = 0.0
            
            # ì œëª©ì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš° í° ë³´ë„ˆìŠ¤
            for keyword in query_keywords:
                if keyword in title:
                    keyword_boost += 0.3
                if keyword in category:
                    keyword_boost += 0.2
            
            # íŠ¹ë³„ í‚¤ì›Œë“œ ì¶”ê°€ ë¶€ìŠ¤íŒ… (ë²•ë¥  ìš©ì–´)
            special_keywords = {
                'íš¡ë ¹': ['íš¡ë ¹', 'ë°°ì„'],
                'ì‚¬ê¸°': ['ì‚¬ê¸°', 'í¸ì·¨'],
                'êµí†µì‚¬ê³ ': ['êµí†µì‚¬ê³ ', 'êµí†µ'],
                'ì†í•´ë°°ìƒ': ['ì†í•´ë°°ìƒ', 'ë°°ìƒ'],
                'ê³„ì•½': ['ê³„ì•½', 'ì•½ì •'],
                'ì´í˜¼': ['ì´í˜¼', 'í˜¼ì¸'],
                'ìƒì†': ['ìƒì†', 'ìœ ì‚°']
            }
            
            for main_keyword, related_keywords in special_keywords.items():
                if main_keyword in query.lower():
                    for related in related_keywords:
                        if related in title or related in category:
                            keyword_boost += 0.4
                            break
            
            # ìµœì¢… ì ìˆ˜ = Cross-encoder ì ìˆ˜ + í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ…
            final_score = base_score + keyword_boost
            
            doc['score'] = final_score
            doc['base_score'] = base_score  # ë””ë²„ê¹…ìš©
            doc['keyword_boost'] = keyword_boost  # ë””ë²„ê¹…ìš©
            scored_results.append(doc)

        reranked_results = sorted(scored_results, key=lambda x: x['score'], reverse=True)
        
        # ì‚¬ìš©ìê°€ ìš”ì²­í•œ ê°œìˆ˜ë§Œí¼ ë°˜í™˜
        actual_return_size = min(requested_size, len(reranked_results))
        top_results = reranked_results[:actual_return_size]
        
        # ë””ë²„ê¹… ë¡œê·¸ ê°œì„ 
        logger.info(f"Reranked {len(reranked_results)} cases with keyword boosting, returning top {len(top_results)} (requested: {requested_size})")
        if top_results:
            logger.info(f"Top result: '{top_results[0].get('title', 'N/A')}' (score: {top_results[0]['score']:.3f} = base: {top_results[0]['base_score']:.3f} + boost: {top_results[0]['keyword_boost']:.3f})")
        
        return top_results
