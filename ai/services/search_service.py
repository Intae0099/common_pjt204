import psycopg2
from pgvector.psycopg2 import register_vector
import os
from dotenv import load_dotenv
from datetime import date

from db.database import get_psycopg2_connection
from utils.logger import setup_logger, get_logger
from utils.exceptions import DatabaseError, SearchError, handle_service_exceptions
from llm.models.embedding_model import EmbeddingModel
from llm.models.cross_encoder_model import CrossEncoderModel

load_dotenv()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë¡œê±° ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
setup_logger()
logger = get_logger(__name__)

class SearchService:
    def __init__(self, embedding_model: EmbeddingModel, cross_encoder_model: CrossEncoderModel):
        self.embedding_model = embedding_model
        self.cross_encoder_model = cross_encoder_model

    async def vector_search(self, query: str, page: int = 1, size: int = 10, use_rerank: bool = True) -> tuple[list[dict], int]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ìš°ì„ ìœ¼ë¡œ í•˜ê³  ë²¡í„° ê²€ìƒ‰ì„ ë³´ì¡°ë¡œ ì‚¬ìš©í•œë‹¤.
        ë²¡í„° ê²€ìƒ‰ ì„±ëŠ¥ì´ ë‚®ì•„ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì„ ë©”ì¸ìœ¼ë¡œ ì „í™˜.
        """
        conn = None
        try:
            conn = get_psycopg2_connection()
            register_vector(conn)

            with conn.cursor() as cur:
                # ì „ì²´ ê°œìˆ˜ (í‚¤ì›Œë“œ ê¸°ë°˜)
                cur.execute("""
                    SELECT COUNT(DISTINCT lc.case_id) 
                    FROM legal_cases lc 
                    WHERE lc.title ILIKE %s OR lc.full_text ILIKE %s OR lc.category ILIKE %s
                """, (f'%{query}%', f'%{query}%', f'%{query}%'))
                keyword_total = cur.fetchone()[0]

                # 1ë‹¨ê³„: í‚¤ì›Œë“œ ê²€ìƒ‰ (ì£¼ë ¥)
                keyword_size = min(size * 2, 30)  # ìš”ì²­ì˜ 2ë°° ë˜ëŠ” ìµœëŒ€ 30ê°œ
                cur.execute("""
                    SELECT lc.case_id, lc.title, lc.decision_date, lc.category, 
                           lc.issue, lc.summary, lc.full_text, lc.statutes,
                           '' as chunk_text,
                           CASE 
                               WHEN lc.title ILIKE %s THEN 3
                               WHEN lc.category ILIKE %s THEN 2  
                               ELSE 1
                           END as keyword_relevance
                    FROM legal_cases lc
                    WHERE lc.title ILIKE %s OR lc.full_text ILIKE %s OR lc.category ILIKE %s
                    ORDER BY keyword_relevance DESC, lc.decision_date DESC
                    LIMIT %s
                    """, (f'%{query}%', f'%{query}%', f'%{query}%', f'%{query}%', f'%{query}%', keyword_size))
                
                keyword_results = [
                    {
                        "case_id": cid,
                        "title": title,
                        "decision_date": decision_date,
                        "category": category,
                        "issue": issue,
                        "summary": summary,
                        "full_text": full_text,
                        "chunk_text": chunk_text,
                        "statutes": statutes,
                        "_source": "keyword"
                    }
                    for cid, title, decision_date, category, issue, summary, full_text, statutes, chunk_text, relevance in cur.fetchall()
                ]

                # 2ë‹¨ê³„: í‚¤ì›Œë“œ ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ë³´ì™„
                initial_results = keyword_results
                if len(keyword_results) < size:
                    logger.info(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ë¶€ì¡± ({len(keyword_results)}ê°œ), ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ë³´ì™„...")
                    
                    query_embedding = self.embedding_model.get_embedding(query)
                    
                    # í‚¤ì›Œë“œ ê²€ìƒ‰ì—ì„œ ë‚˜ì˜¨ case_idë“¤ ì œì™¸
                    exclude_ids = [r["case_id"] for r in keyword_results] if keyword_results else []
                    exclude_clause = ""
                    exclude_params = []
                    
                    if exclude_ids:
                        placeholders = ",".join(["%s"] * len(exclude_ids))
                        exclude_clause = f"AND lc.case_id NOT IN ({placeholders})"
                        exclude_params = exclude_ids
                    
                    vector_size = size - len(keyword_results)
                    vector_query = f"""
                        SELECT lc.case_id, lc.title, lc.decision_date, lc.category, 
                               lc.issue, lc.summary, lc.full_text, lch.chunk_text, lc.statutes
                        FROM legal_chunks lch
                        JOIN legal_cases lc ON lch.case_id = lc.case_id
                        WHERE 1=1 {exclude_clause}
                        ORDER BY lch.embedding <-> %s::vector
                        LIMIT %s
                    """
                    
                    cur.execute(vector_query, exclude_params + [query_embedding, vector_size])
                    
                    vector_results = [
                        {
                            "case_id": cid,
                            "title": title,
                            "decision_date": decision_date,
                            "category": category,
                            "issue": issue,
                            "summary": summary,
                            "full_text": full_text,
                            "chunk_text": chunk_text,
                            "statutes": statutes,
                            "_source": "vector"
                        }
                        for cid, title, decision_date, category, issue, summary, full_text, chunk_text, statutes in cur.fetchall()
                    ]
                    
                    initial_results.extend(vector_results)
                
                # ì „ì²´ countëŠ” í‚¤ì›Œë“œ ê¸°ë°˜ + ë²¡í„° ì „ì²´ë¥¼ í•©ì‚° ì¶”ì •
                if keyword_total > 0:
                    total_count = keyword_total
                else:
                    cur.execute("SELECT COUNT(DISTINCT lc.case_id) FROM legal_chunks lch JOIN legal_cases lc ON lch.case_id = lc.case_id")
                    total_count = cur.fetchone()[0]

            # ğŸ”§ DEBUG: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
            keyword_count = len([r for r in initial_results if r.get("_source") == "keyword"])
            vector_count = len([r for r in initial_results if r.get("_source") == "vector"])
            logger.info(f"[hybrid_search] total={len(initial_results)} (keyword={keyword_count}, vector={vector_count})")

            if use_rerank and len(initial_results) > 1:
                logger.info("Applying reranking to hybrid search results...")
                reranked = self._rerank_cases(query, initial_results, requested_size=size)

                # ë””ë²„ê·¸: ìµœì¢… ê²°ê³¼ ì†ŒìŠ¤ ë¶„í¬
                final_keyword = len([r for r in reranked if r.get("_source") == "keyword"])
                final_vector = len([r for r in reranked if r.get("_source") == "vector"])
                logger.info(f"[final_results] total={len(reranked)} (keyword={final_keyword}, vector={final_vector})")

                return reranked, total_count
            else:
                logger.info("Reranking skipped or insufficient results.")
                # ìš”ì²­ëœ ê°œìˆ˜ë§Œí¼ë§Œ ë°˜í™˜
                return initial_results[:size], total_count

        except psycopg2.Error as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {e}")
            raise DatabaseError(f"íŒë¡€ ê²€ìƒ‰ ì¤‘ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", original_exception=e)
        except Exception as e:
            logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            raise SearchError(f"íŒë¡€ ê²€ìƒ‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", original_exception=e)
        finally:
            if conn:
                conn.close()

    async def high_precision_search(self, query: str, top_k: int = 3) -> list[dict]:
        """
        AI ì‚¬ì „ ìƒë‹´ìš© ê³ ì •ë°€ë„ ê²€ìƒ‰
        3ë‹¨ê³„ í•„í„°ë§ìœ¼ë¡œ ìµœê³  í’ˆì§ˆì˜ ìƒìœ„ kê°œ ê²°ê³¼ë§Œ ë°˜í™˜
        """
        conn = None
        try:
            conn = get_psycopg2_connection()
            register_vector(conn)

            with conn.cursor() as cur:
                # 1ë‹¨ê³„: ëŒ€ëŸ‰ í›„ë³´ ìˆ˜ì§‘ (í‚¤ì›Œë“œ + ë²¡í„°)
                logger.info(f"[high_precision] 1ë‹¨ê³„: ëŒ€ëŸ‰ í›„ë³´ ìˆ˜ì§‘ ì‹œì‘ (query: {query})")
                
                # 1-1. í‚¤ì›Œë“œ ê²€ìƒ‰ (40ê°œ)
                keyword_candidates = []
                cur.execute("""
                    SELECT DISTINCT lc.case_id, lc.title, lc.decision_date, lc.category, 
                           lc.issue, lc.summary, lc.full_text, lc.statutes,
                           CASE 
                               WHEN lc.title ILIKE %s THEN 5
                               WHEN lc.category ILIKE %s THEN 4
                               WHEN lc.issue ILIKE %s THEN 3
                               WHEN lc.summary ILIKE %s THEN 2
                               ELSE 1
                           END as keyword_score
                    FROM legal_cases lc
                    WHERE lc.title ILIKE %s OR lc.category ILIKE %s OR lc.issue ILIKE %s 
                       OR lc.summary ILIKE %s OR lc.full_text ILIKE %s
                    ORDER BY keyword_score DESC, lc.decision_date DESC
                    LIMIT 40
                    """, tuple([f'%{query}%'] * 9))
                
                for row in cur.fetchall():
                    keyword_candidates.append({
                        "case_id": row[0],
                        "title": row[1],
                        "decision_date": row[2],
                        "category": row[3],
                        "issue": row[4],
                        "summary": row[5],
                        "full_text": row[6],
                        "statutes": row[7],
                        "chunk_text": "",
                        "_source": "keyword",
                        "_score": row[8]
                    })
                
                # 1-2. ë²¡í„° ê²€ìƒ‰ (20ê°œ, í‚¤ì›Œë“œ ê²°ê³¼ ì œì™¸)
                keyword_case_ids = [c["case_id"] for c in keyword_candidates]
                vector_candidates = []
                
                if len(keyword_case_ids) < 50:  # í‚¤ì›Œë“œ ê²°ê³¼ê°€ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ë²¡í„°ë¡œ ë³´ì™„
                    query_embedding = self.embedding_model.get_embedding(query)
                    
                    exclude_clause = ""
                    exclude_params = []
                    if keyword_case_ids:
                        placeholders = ",".join(["%s"] * len(keyword_case_ids))
                        exclude_clause = f"AND lc.case_id NOT IN ({placeholders})"
                        exclude_params = keyword_case_ids
                    
                    vector_query = f"""
                        SELECT lc.case_id, lc.title, lc.decision_date, lc.category, 
                               lc.issue, lc.summary, lc.full_text, lch.chunk_text, lc.statutes,
                               lch.embedding <-> %s::vector as distance
                        FROM legal_chunks lch
                        JOIN legal_cases lc ON lch.case_id = lc.case_id
                        WHERE 1=1 {exclude_clause}
                        ORDER BY lch.embedding <-> %s::vector
                        LIMIT 20
                    """
                    
                    cur.execute(vector_query, exclude_params + [query_embedding, query_embedding])
                    
                    for row in cur.fetchall():
                        vector_candidates.append({
                            "case_id": row[0],
                            "title": row[1],
                            "decision_date": row[2],
                            "category": row[3],
                            "issue": row[4],
                            "summary": row[5],
                            "full_text": row[6],
                            "chunk_text": row[7],
                            "statutes": row[8],
                            "_source": "vector",
                            "_score": 1.0 / (1.0 + row[9])  # distance -> similarity ë³€í™˜
                        })

                # 2ë‹¨ê³„: ì •í™•ë„ í•„í„°ë§
                all_candidates = keyword_candidates + vector_candidates
                logger.info(f"[high_precision] 1ë‹¨ê³„ ì™„ë£Œ: {len(all_candidates)}ê°œ í›„ë³´ (keyword: {len(keyword_candidates)}, vector: {len(vector_candidates)})")
                
                if not all_candidates:
                    logger.warning("[high_precision] í›„ë³´ê°€ ì—†ì–´ ë¹ˆ ê²°ê³¼ ë°˜í™˜")
                    return []
                
                # 2ë‹¨ê³„: ê´€ë ¨ì„± ì ìˆ˜ ì¬ê³„ì‚° ë° í•„í„°ë§
                logger.info("[high_precision] 2ë‹¨ê³„: ì •í™•ë„ í•„í„°ë§ ì‹œì‘")
                
                filtered_candidates = []
                query_lower = query.lower()
                
                for candidate in all_candidates:
                    title = (candidate.get("title") or "").lower()
                    category = (candidate.get("category") or "").lower()
                    
                    # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
                    relevance_score = 0.0
                    
                    # ì§ì ‘ í‚¤ì›Œë“œ ë§¤ì¹­
                    if query_lower in title:
                        relevance_score += 3.0
                    if query_lower in category:
                        relevance_score += 2.0
                    
                    # ë²•ë¥  ìš©ì–´ íŠ¹ë³„ ë§¤ì¹­
                    legal_term_map = {
                        "íš¡ë ¹": ["íš¡ë ¹", "ë°°ì„", "íŠ¹ì •ê²½ì œë²”ì£„"],
                        "ì‚¬ê¸°": ["ì‚¬ê¸°", "í¸ì·¨", "ê¸°ë§"],
                        "êµí†µì‚¬ê³ ": ["êµí†µì‚¬ê³ ", "êµí†µ", "ìë™ì°¨"],
                        "ì†í•´ë°°ìƒ": ["ì†í•´ë°°ìƒ", "ë°°ìƒ", "í”¼í•´"],
                        "ê³„ì•½": ["ê³„ì•½", "ì•½ì •", "í•©ì˜"],
                        "ì´í˜¼": ["ì´í˜¼", "í˜¼ì¸", "ê°€ì‚¬"],
                        "ìƒì†": ["ìƒì†", "ìœ ì‚°", "ìœ ì–¸"]
                    }
                    
                    for main_term, related_terms in legal_term_map.items():
                        if main_term in query_lower:
                            for term in related_terms:
                                if term in title:
                                    relevance_score += 2.0
                                    break
                    
                    # ê¸°ë³¸ ì ìˆ˜ ì¶”ê°€
                    relevance_score += candidate.get("_score", 0.0)
                    
                    # ìµœì†Œ ê´€ë ¨ì„± ì„ê³„ê°’ ì ìš©
                    if relevance_score >= 1.0:  # ìµœì†Œ ê´€ë ¨ì„± ë³´ì¥
                        candidate["_relevance"] = relevance_score
                        filtered_candidates.append(candidate)
                
                # ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì •ë ¬
                filtered_candidates.sort(key=lambda x: x["_relevance"], reverse=True)
                
                # ìƒìœ„ 15ê°œë§Œ ìœ ì§€
                filtered_candidates = filtered_candidates[:15]
                logger.info(f"[high_precision] 2ë‹¨ê³„ ì™„ë£Œ: {len(filtered_candidates)}ê°œ í•„í„°ë§ë¨")
                
                if not filtered_candidates:
                    logger.warning("[high_precision] í•„í„°ë§ í›„ ê²°ê³¼ ì—†ìŒ")
                    return []
                
                # 3ë‹¨ê³„: Cross-encoder ìµœì¢… ì„ ë³„
                logger.info(f"[high_precision] 3ë‹¨ê³„: ìµœì¢… {top_k}ê°œ ì„ ë³„")
                
                if len(filtered_candidates) <= top_k:
                    final_results = filtered_candidates
                else:
                    # Cross-encoderë¡œ ìµœì¢… ìˆœìœ„ ê²°ì •
                    final_results = self._rerank_cases(query, filtered_candidates, requested_size=top_k)
                
                logger.info(f"[high_precision] ì™„ë£Œ: {len(final_results)}ê°œ ë°˜í™˜")
                
                # ë””ë²„ê·¸ ë¡œê·¸
                if final_results:
                    top_title = final_results[0].get("title", "N/A")
                    top_source = final_results[0].get("_source", "N/A")
                    top_relevance = final_results[0].get("_relevance", 0)
                    logger.info(f"[high_precision] Top result: '{top_title}' (source: {top_source}, relevance: {top_relevance:.2f})")
                
                return final_results

        except Exception as e:
            logger.error(f"ê³ ì •ë°€ë„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            raise SearchError(f"ê³ ì •ë°€ë„ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", original_exception=e)
        finally:
            if conn:
                conn.close()

    async def get_case_by_id(self, prec_id: str) -> dict | None:
        """
        íŒë¡€ IDë¡œ íŒë¡€ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
        """
        conn = None
        try:
            conn = get_psycopg2_connection()
            with conn.cursor() as cur:
                cur.execute(
                    """
                    SELECT case_id, title, decision_date, category, issue, summary, statutes, precedents, full_text
                    FROM legal_cases
                    WHERE case_id = %s
                    """,
                    (prec_id,)
                )
                result = cur.fetchone()
                if result:
                    columns = [desc[0] for desc in cur.description]
                    return dict(zip(columns, result))
                return None
        except psycopg2.Error as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {e}")
            raise DatabaseError(f"íŒë¡€ ìƒì„¸ ì¡°íšŒ ì¤‘ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", original_exception=e)
        except Exception as e:
            logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            raise SearchError(f"íŒë¡€ ìƒì„¸ ì¡°íšŒ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", original_exception=e)
        finally:
            if conn:
                conn.close()

    def _rerank_cases(self, query: str, initial_results: list[dict], requested_size: int = 10) -> list[dict]:
        """
        Cross-encoder ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¬í‰ê°€í•˜ê³  í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ë¶€ìŠ¤íŒ…í•˜ì—¬ ê´€ë ¨ë„ ìˆœìœ¼ë¡œ ì¬ì •ë ¬í•œë‹¤.
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            initial_results: ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼
            requested_size: ì‚¬ìš©ìê°€ ìš”ì²­í•œ ê²°ê³¼ ê°œìˆ˜
            
        Returns:
            ì¬ì •ë ¬ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (requested_size ë§Œí¼)
        """
        if not initial_results:
            return []

        # 1. Cross-encoder ì ìˆ˜ ê³„ì‚°
        documents_to_rerank = []
        for i, doc in enumerate(initial_results):
            # ì œëª© + ì¹´í…Œê³ ë¦¬ + chunk_textë¥¼ ì¡°í•©í•˜ì—¬ ë” í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
            title = doc.get('title', '') or ''
            category = doc.get('category', '') or ''
            chunk_text = doc.get('chunk_text', '') or ''
            issue = doc.get('issue', '') or ''
            
            # ì œëª©ê³¼ ì¹´í…Œê³ ë¦¬ë¥¼ ìš°ì„ ì‹œí•˜ê³  chunk_textë¥¼ ë³´ì¡°ë¡œ ì‚¬ìš©
            combined_text = f"{title} {category} {issue} {chunk_text}".strip()
            documents_to_rerank.append(combined_text)

        scores = self.cross_encoder_model.get_cross_encoder_scores(query, documents_to_rerank)

        # 2. í‚¤ì›Œë“œ ë§¤ì¹­ ë¶€ìŠ¤íŒ… ì ìš©
        query_keywords = query.lower().split()
        scored_results = []
        
        for i, doc in enumerate(initial_results):
            base_score = scores[i]
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤ ê³„ì‚°
            title = (doc.get('title', '') or '').lower()
            category = (doc.get('category', '') or '').lower()
            
            keyword_boost = 0.0
            
            # ì œëª©ì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš° í° ë³´ë„ˆìŠ¤
            for keyword in query_keywords:
                if keyword in title:
                    keyword_boost += 0.3
                if keyword in category:
                    keyword_boost += 0.2
            
            # íŠ¹ë³„ í‚¤ì›Œë“œ ì¶”ê°€ ë¶€ìŠ¤íŒ… (ë²•ë¥  ìš©ì–´)
            special_keywords = {
                'íš¡ë ¹': ['íš¡ë ¹', 'ë°°ì„'],
                'ì‚¬ê¸°': ['ì‚¬ê¸°', 'í¸ì·¨'],
                'êµí†µì‚¬ê³ ': ['êµí†µì‚¬ê³ ', 'êµí†µ'],
                'ì†í•´ë°°ìƒ': ['ì†í•´ë°°ìƒ', 'ë°°ìƒ'],
                'ê³„ì•½': ['ê³„ì•½', 'ì•½ì •'],
                'ì´í˜¼': ['ì´í˜¼', 'í˜¼ì¸'],
                'ìƒì†': ['ìƒì†', 'ìœ ì‚°']
            }
            
            for main_keyword, related_keywords in special_keywords.items():
                if main_keyword in query.lower():
                    for related in related_keywords:
                        if related in title or related in category:
                            keyword_boost += 0.4
                            break
            
            # ìµœì¢… ì ìˆ˜ = Cross-encoder ì ìˆ˜ + í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ…
            final_score = base_score + keyword_boost
            
            doc['score'] = final_score
            doc['base_score'] = base_score  # ë””ë²„ê¹…ìš©
            doc['keyword_boost'] = keyword_boost  # ë””ë²„ê¹…ìš©
            scored_results.append(doc)

        reranked_results = sorted(scored_results, key=lambda x: x['score'], reverse=True)
        
        # ì‚¬ìš©ìê°€ ìš”ì²­í•œ ê°œìˆ˜ë§Œí¼ ë°˜í™˜
        actual_return_size = min(requested_size, len(reranked_results))
        top_results = reranked_results[:actual_return_size]
        
        # ë””ë²„ê¹… ë¡œê·¸ ê°œì„ 
        logger.info(f"Reranked {len(reranked_results)} cases with keyword boosting, returning top {len(top_results)} (requested: {requested_size})")
        if top_results:
            logger.info(f"Top result: '{top_results[0].get('title', 'N/A')}' (score: {top_results[0]['score']:.3f} = base: {top_results[0]['base_score']:.3f} + boost: {top_results[0]['keyword_boost']:.3f})")
        
        return top_results
