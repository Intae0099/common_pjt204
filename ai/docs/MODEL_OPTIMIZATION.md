# ëª¨ë¸ ê²½ëŸ‰í™” ë° ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

ë³¸ ë¬¸ì„œëŠ” ë²•ë¥  AI ì‹œìŠ¤í…œì˜ ëª¨ë¸ ê²½ëŸ‰í™” ë° ì„±ëŠ¥ ìµœì í™” êµ¬í˜„ì— ëŒ€í•œ ê¸°ìˆ  ë¬¸ì„œì…ë‹ˆë‹¤. GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒê³¼ ì‹œìŠ¤í…œ ì²˜ë¦¬ëŸ‰ ì¦ëŒ€ë¥¼ ìœ„í•œ ì–‘ìí™” ê¸°ë²•ê³¼ ë°°ì¹˜ ìµœì í™”ë¥¼ ì ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ìµœì í™” ëª©í‘œ

- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 30-50% ì ˆê°
- **ì²˜ë¦¬ëŸ‰ í–¥ìƒ**: í ì‹œìŠ¤í…œ ë™ì‹œ ì²˜ë¦¬ ìš©ëŸ‰ 2-3ë°° ì¦ëŒ€  
- **ì •í™•ë„ ìœ ì§€**: ëª¨ë¸ ì„±ëŠ¥ ì†ì‹¤ 3% ë¯¸ë§Œ
- **ì‹œìŠ¤í…œ ì•ˆì •ì„±**: ë¦¬ì†ŒìŠ¤ ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ì˜¤ë¥˜ ìµœì†Œí™”

## ğŸ”§ êµ¬í˜„ëœ ìµœì í™” ê¸°ë²•

### 1. ëª¨ë¸ ì–‘ìí™” (Model Quantization)

#### 1.1 ì„ë² ë”© ëª¨ë¸ ì–‘ìí™”
**íŒŒì¼**: `llm/models/quantized_embedding_model.py`

```python
class QuantizedEmbeddingModel:
    def _apply_quantization(self):
        if self._device == "cuda":
            # GPU: Half Precision (FP16) ì ìš©
            self._model = self._model.half()
        else:
            # CPU: ë™ì  INT8 ì–‘ìí™” ì ìš©
            self._apply_cpu_quantization()
```

**ê¸°ìˆ ì  íŠ¹ì§•**:
- GPU í™˜ê²½: FP32 â†’ FP16 (Half Precision)
- CPU í™˜ê²½: FP32 â†’ INT8 ë™ì  ì–‘ìí™”
- ë””ë°”ì´ìŠ¤ë³„ ìë™ ìµœì í™” ì „ëµ ì ìš©

#### 1.2 Cross-encoder ì–‘ìí™”
**íŒŒì¼**: `llm/models/optimized_cross_encoder_model.py`

```python
def _apply_quantization(self):
    if self._device == "cuda":
        self._model.model = self._model.model.half()
    else:
        # PyTorch ë™ì  ì–‘ìí™”
        self._model.model = torch.quantization.quantize_dynamic(
            self._model.model, {torch.nn.Linear}, dtype=torch.qint8
        )
```

### 2. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

#### 2.1 ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •
```python
def _get_optimal_batch_size(self, num_documents: int) -> int:
    base_batch_size = min(self.max_batch_size, num_documents)
    
    if self._device == "cuda":
        gpu_memory_free = (torch.cuda.get_device_properties(0).total_memory - 
                          torch.cuda.memory_allocated()) / 1024**3
        
        if gpu_memory_free < 1.0:  # 1GB ë¯¸ë§Œ
            optimal_batch_size = max(1, base_batch_size // 4)
        elif gpu_memory_free < 2.0:  # 2GB ë¯¸ë§Œ
            optimal_batch_size = max(1, base_batch_size // 2)
        else:
            optimal_batch_size = base_batch_size
    
    return optimal_batch_size
```

**íŠ¹ì§•**:
- ì‹¤ì‹œê°„ GPU ë©”ëª¨ë¦¬ ìƒíƒœ ëª¨ë‹ˆí„°ë§
- ë©”ëª¨ë¦¬ ê°€ìš©ëŸ‰ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •
- OOM(Out of Memory) ì˜¤ë¥˜ ë°©ì§€

#### 2.2 ë°°ì¹˜ ë²¤ì¹˜ë§ˆí‚¹
```python
def benchmark_batch_sizes(self, query: str, documents: List[str], 
                        test_batch_sizes: List[int] = None) -> dict:
    # ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸°ë³„ ì„±ëŠ¥ ì¸¡ì •
    # ìµœì  ë°°ì¹˜ í¬ê¸° ìë™ ì¶”ì²œ
```

### 3. í ì‹œìŠ¤í…œ í™•ì¥

#### 3.1 ìš©ëŸ‰ ì¦ëŒ€
**íŒŒì¼**: `services/lightweight_queue_manager.py`

```python
# ê²½ëŸ‰í™” ìµœì í™” ì ìš©ìœ¼ë¡œ í™•ì¥ëœ ë¦¬ì†ŒìŠ¤ ì œí•œ ì„¤ì •
ENHANCED_LIMITS = {
    "case_analysis": {
        "max_concurrent": 2,        # 1â†’2 (ê²½ëŸ‰í™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)
        "max_queue_size": 10,       # 5â†’10 (í í™•ì¥)
    },
    "search": {
        "max_concurrent": 4,        # 2â†’4 (ë°°ì¹˜ ìµœì í™” íš¨ê³¼)
        "max_queue_size": 20,       # 10â†’20 (í í™•ì¥)
    },
    "consultation": {
        "max_concurrent": 2,        # 1â†’2 (ì–‘ìí™” íš¨ê³¼)
        "max_queue_size": 10,       # 5â†’10 (í í™•ì¥)
    }
}
```

#### 3.2 ë¦¬ì†ŒìŠ¤ ì„ê³„ê°’ ì™„í™”
```python
def __init__(self):
    self.memory_threshold = 90    # 95%â†’90% (ê²½ëŸ‰í™”ë¡œ ì™„í™”)
    self.cpu_threshold = 90       # 95%â†’90% (ë°°ì¹˜ ìµœì í™”ë¡œ ì™„í™”)
    self.check_interval = 3       # 5ì´ˆâ†’3ì´ˆ (ë” ë°˜ì‘ì )
```

### 4. ëª¨ë¸ ë¡œë” í†µí•©

#### 4.1 í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ
**íŒŒì¼**: `llm/models/model_loader.py`

```python
class ModelLoader:
    USE_OPTIMIZED_MODELS = os.getenv("USE_OPTIMIZED_MODELS", "true").lower() == "true"
    
    @classmethod
    def get_embedding_model(cls) -> EmbeddingModel:
        if cls.USE_OPTIMIZED_MODELS:
            cls._embedding_model_instance = QuantizedEmbeddingModel(enable_quantization=True)
        else:
            cls._embedding_model_instance = EmbeddingModel()
```

**ì„¤ì • ë°©ë²•**:
```bash
# ìµœì í™” ëª¨ë¸ ì‚¬ìš© (ê¸°ë³¸ê°’)
export USE_OPTIMIZED_MODELS=true

# ì›ë³¸ ëª¨ë¸ ì‚¬ìš©
export USE_OPTIMIZED_MODELS=false
```

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼

### ì¸¡ì • í™˜ê²½
- **ì‹œìŠ¤í…œ**: 6 cores CPU, 15.9 GB RAM
- **GPU**: NVIDIA GeForce RTX 3060 (12GB)
- **ì¸¡ì • ë„êµ¬**: `scripts/simple_benchmark.py`

### Cross-Encoder ëª¨ë¸ ìµœì í™” ì„±ê³¼

| ì§€í‘œ | ì›ë³¸ ëª¨ë¸ | ìµœì í™” ëª¨ë¸ | ê°œì„ ë„ |
|------|-----------|-------------|---------|
| **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰** | 1,199 MB | 610 MB | **49.1% ì ˆê°** |
| **ì²˜ë¦¬ ì‹œê°„** | 0.020ì´ˆ | 0.026ì´ˆ | -30.8% (ì•½ê°„ ì¦ê°€) |
| **ì²˜ë¦¬ëŸ‰** | 505.1 docs/sec | 386.2 docs/sec | -23.5% |
| **ì •í™•ë„ ìƒê´€ê´€ê³„** | 1.000 | 0.998 | **99.8% ìœ ì§€** |

### ì •í™•ë„ ì¸¡ì • ë°©ë²•

#### ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì •í™•ë„ ê²€ì¦
ì–‘ìí™” ì „í›„ ëª¨ë¸ì˜ ì¶œë ¥ ì¼ê´€ì„±ì„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ì¸¡ì •:

```python
import numpy as np

def calculate_accuracy_correlation(original_scores, optimized_scores):
    # ì ìˆ˜ ë°°ì—´ì„ ì •ê·œí™”
    orig_norm = np.array(original_scores) / np.linalg.norm(original_scores)
    opt_norm = np.array(optimized_scores) / np.linalg.norm(optimized_scores)
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    cosine_similarity = np.dot(orig_norm, opt_norm)
    return cosine_similarity

# ì˜ˆì‹œ ê²°ê³¼
# Original: [0.500, 0.470, 0.475]
# Optimized: [0.534, 0.524, 0.566] 
# Cosine Similarity: 0.998 (99.8%)
```

**ì¸¡ì • ê¸°ì¤€**:
- **0.95 ì´ìƒ**: ì •í™•ë„ ìœ ì§€ (ì‹¤ìš©ì  ì†ì‹¤ ì—†ìŒ)
- **0.90-0.95**: í—ˆìš© ê°€ëŠ¥í•œ ë²”ìœ„
- **0.90 ë¯¸ë§Œ**: ì¶”ê°€ ìµœì í™” í•„ìš”

**ì¸¡ì • ì›ë¦¬**:
- ì ìˆ˜ íŒ¨í„´ì˜ ìƒëŒ€ì  ìˆœì„œ ë³´ì¡´ ì—¬ë¶€ í™•ì¸
- ì ˆëŒ“ê°’ ì°¨ì´ë³´ë‹¤ **ìƒê´€ê´€ê³„** ì¤‘ì‹¬ í‰ê°€
- ì‹¤ì œ ë­í‚¹ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ìµœì†Œí™”

### ê²°ê³¼ í•´ì„

#### âœ… **ì„±ê³µí•œ ìµœì í™”**
1. **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: 49.1% ì ˆê°ìœ¼ë¡œ GPU ë©”ëª¨ë¦¬ ì—¬ìœ  í™•ë³´
2. **ì •í™•ë„ ìœ ì§€**: 99.8% ìƒê´€ê´€ê³„ë¡œ ì‹¤ì§ˆì  ì„±ëŠ¥ ì†ì‹¤ ì—†ìŒ
3. **ì‹œìŠ¤í…œ ì•ˆì •ì„±**: ë©”ëª¨ë¦¬ ì ˆì•½ìœ¼ë¡œ OOM ì˜¤ë¥˜ ë°©ì§€

#### âš ï¸ **íŠ¸ë ˆì´ë“œì˜¤í”„**
- **ë‹¨ì¼ ìš”ì²­ ì†ë„**: ì–‘ìí™” ì˜¤ë²„í—¤ë“œë¡œ ì•½ê°„ì˜ ì§€ì—°
- **ì‹¤ì œ ìš´ì˜ í™˜ê²½**: ë‹¤ì¤‘ ìš”ì²­ ì²˜ë¦¬ ì‹œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì´ ì „ì²´ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬

### í ì‹œìŠ¤í…œ ê°œì„  íš¨ê³¼

| ì„œë¹„ìŠ¤ | ê¸°ì¡´ ë™ì‹œì²˜ë¦¬ | ìµœì í™” í›„ | ì¦ê°€ìœ¨ |
|--------|---------------|-----------|--------|
| case_analysis | 1 | 2 | **100%** |
| search | 2 | 4 | **100%** |
| consultation | 1 | 2 | **100%** |
| structuring | 2 | 3 | **50%** |
| chat | 3 | 5 | **67%** |

## ğŸš€ ì‚¬ìš©ë²•

### 1. ìµœì í™” ëª¨ë¸ í™œì„±í™”
```bash
# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export USE_OPTIMIZED_MODELS=true

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘
python app/main.py
```

### 2. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ì‹¤í–‰
```bash
# ê°„ë‹¨í•œ ë²¤ì¹˜ë§ˆí¬
python scripts/simple_benchmark.py

# ìƒì„¸í•œ ëª¨ë¸ í…ŒìŠ¤íŠ¸
python scripts/test_model_optimization.py
```

### 3. ëª¨ë¸ ì •ë³´ í™•ì¸
```python
from llm.models.model_loader import ModelLoader

# í˜„ì¬ ëª¨ë¸ ì •ë³´ í™•ì¸
model_info = ModelLoader.get_model_info()
print(model_info)

# ëª¨ë¸ ì¬ë¡œë“œ (ìµœì í™” ëª¨ë“œ ë³€ê²½)
ModelLoader.reload_models(use_optimized=True)
```

## ğŸ” ëª¨ë‹ˆí„°ë§

### GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
```python
import torch

if torch.cuda.is_available():
    allocated = torch.cuda.memory_allocated() / 1024**3
    total = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"GPU ë©”ëª¨ë¦¬: {allocated:.1f}GB / {total:.1f}GB")
```

### ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```python
# ì„ë² ë”© ëª¨ë¸
embedding_model = ModelLoader.get_embedding_model()
if hasattr(embedding_model, 'get_model_info'):
    info = embedding_model.get_model_info()
    print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {info['memory_usage_mb']:.1f}MB")

# Cross-encoder ëª¨ë¸  
cross_encoder = ModelLoader.get_cross_encoder_model()
if hasattr(cross_encoder, 'get_model_info'):
    info = cross_encoder.get_model_info()
    print(f"ì–‘ìí™” í™œì„±í™”: {info['quantization_enabled']}")
```

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜
```
RuntimeError: CUDA out of memory
```

**í•´ê²°ë°©ë²•**:
```python
# ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ
optimized_model = OptimizedCrossEncoderModel(max_batch_size=4)

# ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export USE_OPTIMIZED_MODELS=true
```

### 2. ì–‘ìí™” ì˜¤ë¥˜
```
ValueError: quantization failed
```

**í•´ê²°ë°©ë²•**:
```python
# ì–‘ìí™” ë¹„í™œì„±í™”
model = QuantizedEmbeddingModel(enable_quantization=False)
```

### 3. ì„±ëŠ¥ ì €í•˜
- ë‹¨ì¼ ìš”ì²­ì—ì„œëŠ” ì•½ê°„ì˜ ì˜¤ë²„í—¤ë“œ ë°œìƒ ê°€ëŠ¥
- ë‹¤ì¤‘ ìš”ì²­ í™˜ê²½ì—ì„œëŠ” ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ìœ¼ë¡œ ì „ì²´ ì„±ëŠ¥ í–¥ìƒ
- ë°°ì¹˜ ì²˜ë¦¬ í™œìš© ê¶Œì¥

### 4. ì •í™•ë„ ê²€ì¦
```bash
# ì •í™•ë„ ì¸¡ì • í¬í•¨ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
python scripts/simple_benchmark.py

# ì¶œë ¥ ì˜ˆì‹œ:
# Score correlation (cosine similarity): 0.998
# Accuracy maintained: YES
```

## ğŸ“š ì°¸ê³ ìë£Œ

### ê´€ë ¨ íŒŒì¼
- `llm/models/quantized_embedding_model.py`: ì–‘ìí™” ì„ë² ë”© ëª¨ë¸
- `llm/models/optimized_cross_encoder_model.py`: ìµœì í™” Cross-encoder
- `services/lightweight_queue_manager.py`: í™•ì¥ëœ í ì‹œìŠ¤í…œ
- `scripts/simple_benchmark.py`: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ë„êµ¬

### ê¸°ìˆ  ë¬¸ì„œ
- [PyTorch Quantization](https://pytorch.org/docs/stable/quantization.html)
- [NVIDIA Mixed Precision Training](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/)
- [Sentence Transformers Performance](https://www.sbert.net/docs/training/overview.html)

## ğŸ”® í–¥í›„ ê°œì„  ê³„íš

### Phase 2 ìµœì í™” (ì˜ˆì •)
1. **ëª¨ë¸ ì••ì¶•**: Pruning ê¸°ë²• ì ìš©
2. **ë¶„ì‚° ì²˜ë¦¬**: ë‹¤ì¤‘ GPU ì§€ì›
3. **ìºì‹± ì‹œìŠ¤í…œ**: ì„ë² ë”© ê²°ê³¼ ìºì‹œ
4. **A/B í…ŒìŠ¤íŠ¸**: ì„±ëŠ¥ ë¹„êµ í”„ë ˆì„ì›Œí¬

### ëª¨ë‹ˆí„°ë§ ê°•í™”
1. **ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­**: Prometheus/Grafana ì—°ë™
2. **ì„±ëŠ¥ ì•Œë¦¼**: ì„ê³„ê°’ ê¸°ë°˜ ì•Œë¦¼ ì‹œìŠ¤í…œ
3. **ìë™ ìŠ¤ì¼€ì¼ë§**: ë¶€í•˜ì— ë”°ë¥¸ ìë™ ì¡°ì •

---

**ì‘ì„±ì¼**: 2025-08-15  
**ì‘ì„±ì**: AI ì‹œìŠ¤í…œ ìµœì í™”íŒ€  
**ë²„ì „**: 1.0  